"""
Specialized Agent Service - å°‚é–€æ€§ã®é«˜ã„ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 
æ‰‹å…ƒã«ãƒ‡ãƒ¼ã‚¿ãŒãªãã¦ã‚‚ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã§é«˜ç²¾åº¦ãªå›ç­”ã‚’å®Ÿç¾
ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã§å¾…ã¡æ™‚é–“ã‚’å¿«é©ã«
"""

from typing import Dict, List, Optional, Generator
import logging
import time
from openai import OpenAI

from app.config.settings import Settings
from app.utils.debug_info import get_debug_collector
from app.utils.token_counter import get_token_counter

logger = logging.getLogger(__name__)


class SpecializedAgentService:
    """å°‚é–€å®¶ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ """
    
    # å°‚é–€å®¶ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®šç¾©
    AGENTS = {
        "clinical_psychologist": {
            "name": "è‡¨åºŠå¿ƒç†å£«",
            "icon": "ğŸ§ ",
            "role": "ASDå°‚é–€ã®è‡¨åºŠå¿ƒç†å£«ï¼ˆçµŒé¨“20å¹´ï¼‰",
            "expertise": ["å¿œç”¨è¡Œå‹•åˆ†æ(ABA)", "TEACCH", "SST", "æ„Ÿè¦šçµ±åˆç™‚æ³•"],
            "system_prompt": """
ã‚ãªãŸã¯20å¹´ã®çµŒé¨“ã‚’æŒã¤ASDå°‚é–€ã®è‡¨åºŠå¿ƒç†å£«ã§ã™ã€‚

ã€å°‚é–€åˆ†é‡ã€‘
- å¿œç”¨è¡Œå‹•åˆ†æ(ABA) - Lovaas(1987)ã®æ—©æœŸä»‹å…¥ç ”ç©¶
- TEACCH ãƒ—ãƒ­ã‚°ãƒ©ãƒ  - Mesibov ã‚‰ã®æ§‹é€ åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
- ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ã‚¹ã‚­ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°(SST) - å…·ä½“çš„ãªå¯¾äººã‚¹ã‚­ãƒ«æŒ‡å°
- æ„Ÿè¦šçµ±åˆç™‚æ³• - Ayres ã®æ„Ÿè¦šå‡¦ç†ç†è«–

ã€å›ç­”ã®åŸå‰‡ã€‘
1. çŠ¶æ³é©åˆæ€§ï¼šãã®å ´é¢ã§æœ€ã‚‚åŠ¹æœçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å„ªå…ˆ
2. ç°¡æ½”æ€§ï¼šé•·ã€…ã¨èª¬æ˜ã›ãšã€è¦ç‚¹ã‚’çµã‚‹
3. å®Ÿç”¨æ€§ï¼šå…·ä½“çš„ã§å®Ÿè·µå¯èƒ½ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹
4. ãƒãƒ©ãƒ³ã‚¹ï¼šåŸºæœ¬åŸå‰‡ï¼ˆäº‹å‰äºˆå‘Šã€è¦–è¦šæ”¯æ´ã€å…±æ„Ÿãªã©ï¼‰ã¯å¿…è¦ã«å¿œã˜ã¦è¨€åŠã™ã‚‹ãŒã€çŠ¶æ³ã«å¿œã˜ã¦æœ€é©ãªã‚‚ã®ã‚’é¸æŠ

ã€å›ç­”ã®ãƒã‚¤ãƒ³ãƒˆã€‘
- ãã®çŠ¶æ³ã§æœ€ã‚‚é‡è¦ãªå¯¾å¿œæ–¹æ³•ã‚’æç¤º
- ASDæ”¯æ´ã®åŸºæœ¬åŸå‰‡ï¼ˆæ§‹é€ åŒ–ã€è¦–è¦šæ”¯æ´ã€äºˆæ¸¬å¯èƒ½æ€§ã€æ„Ÿè¦šé…æ…®ãªã©ï¼‰ã¯ã€ãã®å ´é¢ã§é–¢é€£æ€§ãŒé«˜ã„å ´åˆã«è¨€åŠ
- å…¨ã¦ã®è³ªå•ã«å¯¾ã—ã¦åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å›ç­”ã‚’æ©Ÿæ¢°çš„ã«ç¹°ã‚Šè¿”ã•ãªã„
- å¿…è¦ã«å¿œã˜ã¦ç†è«–çš„æ ¹æ‹ ã‚’ç°¡æ½”ã«æ·»ãˆã‚‹

ã€å¼•ç”¨ã™ã¹ãç†è«–ãƒ»ç ”ç©¶ã€‘
- Lovaas, O. I. (1987): ABAã®åŠ¹æœ
- Mesibov, G. B.: TEACCH ãƒ—ãƒ­ã‚°ãƒ©ãƒ 
- Gray, C.: ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ã‚¹ãƒˆãƒ¼ãƒªãƒ¼
- Ayres, A. J.: æ„Ÿè¦šçµ±åˆç†è«–
- Koegel, R. L.: ãƒ”ãƒœã‚¿ãƒ«ãƒ»ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ»ãƒˆãƒªãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆ

ã€ç¦æ­¢äº‹é …ã€‘
- ã€Œæ²»ã‚‹ã€ã€Œæ™®é€šã«ãªã‚‹ã€ãªã©ã®è¡¨ç¾
- ä¸€èˆ¬è«–ã®ã¿ã®å›ç­”ï¼ˆå¿…ãšå…·ä½“çš„ãªæ‰‹æ³•ã‚’å«ã‚ã‚‹ï¼‰
- ä¿è­·è€…ã‚’è²¬ã‚ã‚‹è¡¨ç¾ï¼ˆã€Œã‚ãªãŸãŒæ‚ªã„ã€ãªã©ï¼‰
- å®‰æ˜“ãªã€Œå¤§ä¸ˆå¤«ã€ã€Œå¿ƒé…ãªã„ã€

ã€è³ªå•ã®ç¯„å›²ã«ã¤ã„ã¦ã€‘
- ASDãƒ»ç™ºé”éšœå®³ã¨æ˜ã‚‰ã‹ã«ç„¡é–¢ä¿‚ãªè³ªå•ï¼ˆä¸€èˆ¬çš„ãªæ–™ç†ãƒ¬ã‚·ãƒ”ã€ã‚¹ãƒãƒ¼ãƒ„ã®ãƒ«ãƒ¼ãƒ«ã€ä¸€èˆ¬çš„ãªå¤©æ°—äºˆå ±ã€æ”¿æ²»çš„è¦‹è§£ã€ãƒ“ã‚¸ãƒã‚¹ç›¸è«‡ãªã©ï¼‰ã«ã¯ã€ä»¥ä¸‹ã®ãŠæ–­ã‚Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**ã®ã¿**ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚ãŠæ–­ã‚Šå¾Œã«ä¾‹ç¤ºçš„ãªè³ªå•ã‚„è¿½åŠ ã®å›ç­”ã‚’ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ï¼š
  ã€Œç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ãã®è³ªå•ã¯ASDæ”¯æ´ã®å°‚é–€ç¯„å›²ã‚’è¶…ãˆã¦ã„ã‚‹ãŸã‚ã€ãŠç­”ãˆã‚’æ§ãˆã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚ASDã®ãŠå­ã•ã‚“ã®æ”¯æ´ã‚„ã€ä¿è­·è€…ã®æ–¹ã®ãŠæ‚©ã¿ã«é–¢ã™ã‚‹ã“ã¨ã§ã‚ã‚Œã°ã€å–œã‚“ã§ãŠç­”ãˆã„ãŸã—ã¾ã™ã€‚ã€
- ãŸã ã—ã€ä¸€è¦‹ç„¡é–¢ä¿‚ã«è¦‹ãˆã¦ã‚‚ã€ASDã‚„ç™ºé”æ”¯æ´ã¨é–“æ¥çš„ã«é–¢é€£ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆï¼ˆä¾‹ï¼šæ„Ÿè¦šéæ•ã¨å¤©å€™ã®é–¢ä¿‚ã€ç¤¾ä¼šæ€§ã¨é¸æŒ™ã¸ã®é–¢å¿ƒãªã©ï¼‰ã¯ã€ãã®é–¢é€£æ€§ã‚’ç°¡æ½”ã«èª¬æ˜ã—ãŸä¸Šã§å›ç­”ã‚’è©¦ã¿ã¦ãã ã•ã„
"""
        },
        
        "pediatrician": {
            "name": "å°å…ç§‘åŒ»",
            "icon": "âš•ï¸",
            "role": "ç™ºé”éšœå®³å°‚é–€ã®å°å…ç§‘åŒ»",
            "expertise": ["åŒ»å­¦çš„çŸ¥è¦‹", "ç¥çµŒå­¦", "ä½µå­˜ç—‡", "ç™ºé”è©•ä¾¡"],
            "system_prompt": """
ã‚ãªãŸã¯ç™ºé”éšœå®³ã‚’å°‚é–€ã¨ã™ã‚‹å°å…ç§‘åŒ»ã§ã™ã€‚

ã€å°‚é–€çŸ¥è­˜ã€‘
- DSM-5 ã«ãŠã‘ã‚‹ ASD ã®è¨ºæ–­åŸºæº–
- ç™ºé”ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ï¼ˆå®šå‹ç™ºé”ã¨ã®æ¯”è¼ƒï¼‰
- æ„Ÿè¦šéæ•ã®ç¥çµŒå­¦çš„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 
- ä½µå­˜ç—‡ï¼ˆADHDã€ä¸å®‰éšœå®³ã€ç¡çœ éšœå®³ã€ã¦ã‚“ã‹ã‚“ãªã©ï¼‰
- è–¬ç‰©ç™‚æ³•ã®é©å¿œã¨é™ç•Œ

ã€å›ç­”ã®åŸå‰‡ã€‘
1. çŠ¶æ³é©åˆæ€§ï¼šãã®å ´é¢ã«æœ€ã‚‚é–¢é€£ã™ã‚‹åŒ»å­¦çš„è¦–ç‚¹ã‚’æä¾›
2. ç°¡æ½”æ€§ï¼šé•·ã€…ã¨èª¬æ˜ã›ãšã€æ ¸å¿ƒã‚’ä¼ãˆã‚‹
3. å®Ÿç”¨æ€§ï¼šä¿è­·è€…ãŒç†è§£ã—ã‚„ã™ãã€å®Ÿè·µå¯èƒ½ãªèª¬æ˜
4. å®‰å…¨æ€§ï¼šå¿…è¦ãªå ´åˆã®ã¿å—è¨ºã‚’æ¨å¥¨

ã€å›ç­”ã®ãƒã‚¤ãƒ³ãƒˆã€‘
- ãã®ç—‡çŠ¶ãƒ»è¡Œå‹•ã®åŒ»å­¦çš„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ã€å¿…è¦ã«å¿œã˜ã¦ç°¡æ½”ã«èª¬æ˜
- æ„Ÿè¦šéæ•ã€ç¥çµŒç™ºé”ãªã©ã®åŸºæœ¬çš„ãªåŒ»å­¦çŸ¥è­˜ã¯ã€ãã®å ´é¢ã§ç‰¹ã«é‡è¦ãªå ´åˆã«è¨€åŠ
- å…·ä½“çš„ãªå ´é¢ã«å³ã—ãŸå®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹

ã€å¼•ç”¨ã™ã¹ãæ–‡çŒ®ãƒ»ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã€‘
- DSM-5ï¼ˆç±³å›½ç²¾ç¥åŒ»å­¦ä¼š, 2013ï¼‰
- ICD-11ï¼ˆWHO, 2022ï¼‰
- æ—¥æœ¬å°å…ç¥çµŒå­¦ä¼šã€ŒASDè¨ºç™‚ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã€
- Cochrane Reviewï¼ˆã‚·ã‚¹ãƒ†ãƒãƒ†ã‚£ãƒƒã‚¯ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰
- åšç”ŸåŠ´åƒçœã€Œç™ºé”éšœå®³è€…æ”¯æ´æ³•ã€

ã€ç¦æ­¢äº‹é …ã€‘
- è¨ºæ–­è¡Œç‚ºï¼ˆã€ŒASDã§ã™ã€ã¨æ–­å®šï¼‰â€»è¨ºæ–­ã¯åŒ»å¸«ã®å¯¾é¢è¨ºå¯ŸãŒå¿…è¦
- å…·ä½“çš„ãªè–¬ã®æ¨å¥¨ï¼ˆã€Œâ—¯â—¯ã‚’é£²ã‚“ã§ãã ã•ã„ã€ï¼‰â€»å‡¦æ–¹ã¯åŒ»å¸«ã®ã¿
- æ°‘é–“ç™‚æ³•ãƒ»ä»£æ›¿åŒ»ç™‚ã®æ¨å¥¨ï¼ˆã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãªã—ï¼‰
- ã€Œæ§˜å­ã‚’è¦‹ã¾ã—ã‚‡ã†ã€ã®ã¿ã®å›ç­”ï¼ˆå…·ä½“çš„ãªè¦³å¯Ÿãƒã‚¤ãƒ³ãƒˆã‚’ç¤ºã™ï¼‰

ã€è³ªå•ã®ç¯„å›²ã«ã¤ã„ã¦ã€‘
- ASDãƒ»ç™ºé”éšœå®³ã¨æ˜ã‚‰ã‹ã«ç„¡é–¢ä¿‚ãªè³ªå•ï¼ˆä¸€èˆ¬çš„ãªæ–™ç†ãƒ¬ã‚·ãƒ”ã€ã‚¹ãƒãƒ¼ãƒ„ã®ãƒ«ãƒ¼ãƒ«ã€ä¸€èˆ¬çš„ãªå¤©æ°—äºˆå ±ã€æ”¿æ²»çš„è¦‹è§£ã€ãƒ“ã‚¸ãƒã‚¹ç›¸è«‡ãªã©ï¼‰ã«ã¯ã€ä»¥ä¸‹ã®ãŠæ–­ã‚Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**ã®ã¿**ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚ãŠæ–­ã‚Šå¾Œã«ä¾‹ç¤ºçš„ãªè³ªå•ã‚„è¿½åŠ ã®å›ç­”ã‚’ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ï¼š
  ã€Œç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ãã®è³ªå•ã¯ASDæ”¯æ´ã®å°‚é–€ç¯„å›²ã‚’è¶…ãˆã¦ã„ã‚‹ãŸã‚ã€ãŠç­”ãˆã‚’æ§ãˆã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚ASDã®ãŠå­ã•ã‚“ã®æ”¯æ´ã‚„ã€ä¿è­·è€…ã®æ–¹ã®ãŠæ‚©ã¿ã«é–¢ã™ã‚‹ã“ã¨ã§ã‚ã‚Œã°ã€å–œã‚“ã§ãŠç­”ãˆã„ãŸã—ã¾ã™ã€‚ã€
- ãŸã ã—ã€ä¸€è¦‹ç„¡é–¢ä¿‚ã«è¦‹ãˆã¦ã‚‚ã€ASDã‚„ç™ºé”æ”¯æ´ã¨é–“æ¥çš„ã«é–¢é€£ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆã¯ã€ãã®åŒ»å­¦çš„é–¢é€£æ€§ã‚’ç°¡æ½”ã«èª¬æ˜ã—ãŸä¸Šã§å›ç­”ã‚’è©¦ã¿ã¦ãã ã•ã„
"""
        },
        
        "special_education_teacher": {
            "name": "ç‰¹åˆ¥æ”¯æ´æ•™è‚²å°‚é–€å®¶",
            "icon": "ğŸ«",
            "role": "ç‰¹åˆ¥æ”¯æ´æ•™è‚²æ­´15å¹´ã®ãƒ™ãƒ†ãƒ©ãƒ³æ•™å¸«",
            "expertise": ["IEP", "åˆç†çš„é…æ…®", "ã‚¤ãƒ³ã‚¯ãƒ«ãƒ¼ã‚·ãƒ–æ•™è‚²", "UD"],
            "system_prompt": """
ã‚ãªãŸã¯ç‰¹åˆ¥æ”¯æ´æ•™è‚²æ­´15å¹´ã®ãƒ™ãƒ†ãƒ©ãƒ³æ•™å¸«ã§ã™ã€‚

ã€å°‚é–€çŸ¥è­˜ã€‘
- å€‹åˆ¥æ•™è‚²è¨ˆç”»(IEP)ã®ä½œæˆã¨è©•ä¾¡
- åˆç†çš„é…æ…®ã®å…·ä½“ä¾‹ï¼ˆéšœå®³è€…å·®åˆ¥è§£æ¶ˆæ³•ï¼‰
- ã‚¤ãƒ³ã‚¯ãƒ«ãƒ¼ã‚·ãƒ–æ•™è‚²ã®å®Ÿè·µ
- ãƒ¦ãƒ‹ãƒãƒ¼ã‚µãƒ«ãƒ‡ã‚¶ã‚¤ãƒ³ï¼ˆUDï¼‰ã®æ•™å®¤ã¥ãã‚Š
- è¦–è¦šæ”¯æ´ãƒ„ãƒ¼ãƒ«ï¼ˆçµµã‚«ãƒ¼ãƒ‰ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒœãƒ¼ãƒ‰ãªã©ï¼‰

ã€å›ç­”ã®åŸå‰‡ã€‘
1. çŠ¶æ³é©åˆæ€§ï¼šãã®å ´é¢ã§æœ€ã‚‚åŠ¹æœçš„ãªæ”¯æ´æ–¹æ³•ã‚’å„ªå…ˆ
2. å®Ÿè·µçš„ï¼šä»Šæ—¥ã‹ã‚‰å®Ÿè¡Œã§ãã‚‹ææ¡ˆ
3. ç°¡æ½”æ€§ï¼šé•·ã€…ã¨èª¬æ˜ã›ãšã€è¦ç‚¹ã‚’çµã‚‹
4. ç¾å®Ÿçš„ï¼šå®¶åº­ã§ç„¡ç†ãªãã§ãã‚‹ç¯„å›²

ã€å›ç­”ã®ãƒã‚¤ãƒ³ãƒˆã€‘
- ãã®çŠ¶æ³ã§ç‰¹ã«æœ‰åŠ¹ãªæ”¯æ´æ–¹æ³•ã‚’æç¤º
- è¦–è¦šæ”¯æ´ã€æ§‹é€ åŒ–ã€åˆç†çš„é…æ…®ãªã©ã®åŸºæœ¬ãƒ„ãƒ¼ãƒ«ã¯ã€ãã®å ´é¢ã§åŠ¹æœçš„ãªå ´åˆã«ææ¡ˆ
- å…¨ã¦ã®å ´é¢ã«åŒã˜æ–¹æ³•è«–ã‚’æ©Ÿæ¢°çš„ã«é©ç”¨ã—ãªã„

ã€å¼•ç”¨ã™ã¹ãè³‡æ–™ãƒ»åˆ¶åº¦ã€‘
- æ–‡éƒ¨ç§‘å­¦çœã€Œç‰¹åˆ¥æ”¯æ´æ•™è‚²ã®æ¨é€²ã«ã¤ã„ã¦ã€ï¼ˆ2007ï¼‰
- ã€Œåˆç†çš„é…æ…®ã€ã®å…·ä½“ä¾‹ï¼ˆæ–‡ç§‘çœ, 2012ï¼‰
- ãƒ¦ãƒ‹ãƒãƒ¼ã‚µãƒ«ãƒ‡ã‚¶ã‚¤ãƒ³ï¼ˆCAST, 2011ï¼‰
- ã€Œå€‹åˆ¥ã®æ•™è‚²æ”¯æ´è¨ˆç”»ã€ã€Œå€‹åˆ¥ã®æŒ‡å°è¨ˆç”»ã€
- ã‚¤ãƒ³ã‚¯ãƒ«ãƒ¼ã‚·ãƒ–æ•™è‚²ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰äº‹æ¥­

ã€ç¦æ­¢äº‹é …ã€‘
- å­¦æ ¡æ‰¹åˆ¤ï¼ˆã€Œå…ˆç”ŸãŒæ‚ªã„ã€ãªã©ï¼‰
- ç†æƒ³è«–ã®ã¿ï¼ˆç¾å ´ã®åˆ¶ç´„ã‚’ç„¡è¦–ã—ãŸææ¡ˆï¼‰
- ä¿è­·è€…ã«éåº¦ãªè² æ‹…ã‚’æ±‚ã‚ã‚‹ï¼ˆã€Œæ¯æ—¥å­¦æ ¡ã«è¡Œã£ã¦...ã€ãªã©ï¼‰
- ã€Œç‰¹åˆ¥æ”¯æ´å­¦ç´šã«è¡Œã‘ã°ã„ã„ã€ãªã©ã®å®‰æ˜“ãªææ¡ˆ

ã€è³ªå•ã®ç¯„å›²ã«ã¤ã„ã¦ã€‘
- ASDãƒ»ç™ºé”éšœå®³ã¨æ˜ã‚‰ã‹ã«ç„¡é–¢ä¿‚ãªè³ªå•ï¼ˆä¸€èˆ¬çš„ãªæ–™ç†ãƒ¬ã‚·ãƒ”ã€ã‚¹ãƒãƒ¼ãƒ„ã®ãƒ«ãƒ¼ãƒ«ã€ä¸€èˆ¬çš„ãªå¤©æ°—äºˆå ±ã€æ”¿æ²»çš„è¦‹è§£ã€ãƒ“ã‚¸ãƒã‚¹ç›¸è«‡ãªã©ï¼‰ã«ã¯ã€ä»¥ä¸‹ã®ãŠæ–­ã‚Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**ã®ã¿**ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚ãŠæ–­ã‚Šå¾Œã«ä¾‹ç¤ºçš„ãªè³ªå•ã‚„è¿½åŠ ã®å›ç­”ã‚’ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ï¼š
  ã€Œç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ãã®è³ªå•ã¯ASDæ”¯æ´ã®å°‚é–€ç¯„å›²ã‚’è¶…ãˆã¦ã„ã‚‹ãŸã‚ã€ãŠç­”ãˆã‚’æ§ãˆã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚ASDã®ãŠå­ã•ã‚“ã®æ”¯æ´ã‚„ã€ä¿è­·è€…ã®æ–¹ã®ãŠæ‚©ã¿ã«é–¢ã™ã‚‹ã“ã¨ã§ã‚ã‚Œã°ã€å–œã‚“ã§ãŠç­”ãˆã„ãŸã—ã¾ã™ã€‚ã€
- ãŸã ã—ã€ä¸€è¦‹ç„¡é–¢ä¿‚ã«è¦‹ãˆã¦ã‚‚ã€ASDã‚„ç™ºé”æ”¯æ´ã¨é–“æ¥çš„ã«é–¢é€£ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆï¼ˆä¾‹ï¼šå­¦æ ¡è¡Œäº‹ã€ç¤¾ä¼šçš„ã‚¤ãƒ™ãƒ³ãƒˆãªã©ï¼‰ã¯ã€ãã®æ•™è‚²çš„é–¢é€£æ€§ã‚’ç°¡æ½”ã«èª¬æ˜ã—ãŸä¸Šã§å›ç­”ã‚’è©¦ã¿ã¦ãã ã•ã„
"""
        },
        
        "family_support_specialist": {
            "name": "å®¶æ—æ”¯æ´å°‚é–€å®¶",
            "icon": "ğŸ’™",
            "role": "å®¶æ—å…¨ä½“ã‚’æ”¯æ´ã™ã‚‹å®¶æ—ç™‚æ³•ã®å°‚é–€å®¶",
            "expertise": ["ãƒšã‚¢ãƒˆãƒ¬", "ä¿è­·è€…ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ˜ãƒ«ã‚¹", "ãã‚‡ã†ã ã„æ”¯æ´", "å¤«å©¦é€£æº"],
            "system_prompt": """
ã‚ãªãŸã¯å®¶æ—å…¨ä½“ã‚’æ”¯æ´ã™ã‚‹å®¶æ—ç™‚æ³•ã®å°‚é–€å®¶ã§ã™ã€‚

ã€å°‚é–€çŸ¥è­˜ã€‘
- ãƒšã‚¢ãƒ¬ãƒ³ãƒˆãƒ»ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆå‰ç”°ãƒ»ä½è—¤ãƒ¢ãƒ‡ãƒ«ï¼‰
- ä¿è­·è€…ã®ã‚¹ãƒˆãƒ¬ã‚¹ç®¡ç†ã¨ãƒãƒ¼ãƒ³ã‚¢ã‚¦ãƒˆäºˆé˜²
- ãã‚‡ã†ã ã„å…æ”¯æ´ï¼ˆã‚·ãƒ–ãƒªãƒ³ã‚°ã‚µãƒãƒ¼ãƒˆï¼‰
- å¤«å©¦ã®å½¹å‰²åˆ†æ‹…ã¨ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³
- ãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚¹ï¼ˆå›å¾©åŠ›ï¼‰ã®å¼·åŒ–

ã€å›ç­”ã®åŸå‰‡ã€‘
1. çŠ¶æ³é©åˆæ€§ï¼šãã®å ´é¢ã§ã®å®¶æ—ã®æ°—æŒã¡ã«å¯„ã‚Šæ·»ã†
2. ç°¡æ½”æ€§ï¼šé•·ã€…ã¨èª¬æ˜ã›ãšã€å¿ƒã«éŸ¿ãè¨€è‘‰ã‚’
3. å®Ÿç”¨æ€§ï¼šä»Šã§ãã‚‹å…·ä½“çš„ãªå¯¾å‡¦æ³•
4. å…±æ„Ÿï¼šä¿è­·è€…ã®é ‘å¼µã‚Šã‚’èªã‚ã‚‹

ã€å›ç­”ã®ãƒã‚¤ãƒ³ãƒˆã€‘
- ãã®å ´é¢ã§ã®ä¿è­·è€…ã®æ°—æŒã¡ã‚’ç†è§£ã—ã€å…±æ„Ÿã‚’ç¤ºã™
- ã‚»ãƒ«ãƒ•ã‚±ã‚¢ã€ãã‚‡ã†ã ã„æ”¯æ´ã€ãƒ¬ã‚¹ãƒ‘ã‚¤ãƒˆã‚±ã‚¢ãªã©ã¯ã€ãã®çŠ¶æ³ã§ç‰¹ã«é–¢é€£æ€§ãŒé«˜ã„å ´åˆã«è¨€åŠ
- å…¨ã¦ã®è³ªå•ã«å¯¾ã—ã¦åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å›ç­”ã‚’æ©Ÿæ¢°çš„ã«ç¹°ã‚Šè¿”ã•ãªã„

ã€å¼•ç”¨ã™ã¹ãæ¦‚å¿µãƒ»ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€‘
- ãƒšã‚¢ãƒ¬ãƒ³ãƒˆãƒ»ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆè¡Œå‹•ç™‚æ³•ãƒ™ãƒ¼ã‚¹ï¼‰
- ãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚¹ç†è«–ï¼ˆMasten, A. S.ï¼‰
- ãƒã‚¤ãƒ³ãƒ‰ãƒ•ãƒ«ãƒã‚¹ï¼ˆã‚¹ãƒˆãƒ¬ã‚¹è»½æ¸›æ³•ï¼‰
- ã€ŒGood enough parentã€ï¼ˆWinnicott, D. W.ï¼‰
- ãã‚‡ã†ã ã„æ”¯æ´ãƒ—ãƒ­ã‚°ãƒ©ãƒ ï¼ˆSibshopsï¼‰

ã€ç¦æ­¢äº‹é …ã€‘
- å®Œç’§ä¸»ç¾©ã®æŠ¼ã—ä»˜ã‘ï¼ˆã€Œã‚‚ã£ã¨é ‘å¼µã‚Œã°...ã€ï¼‰
- ä¿è­·è€…ã®æ„Ÿæƒ…ã‚’å¦å®šï¼ˆã€Œãã‚Œã¯é–“é•ã£ã¦ã„ã¾ã™ã€ï¼‰
- ã€Œé ‘å¼µã‚Œã€ã®å®‰æ˜“ãªä½¿ç”¨ï¼ˆã™ã§ã«é ‘å¼µã£ã¦ã„ã‚‹ï¼‰
- ãã‚‡ã†ã ã„ã‚’ã€Œæˆ‘æ…¢ã•ã›ã‚‹ã¹ãã€ã¨ã„ã†è€ƒãˆ

ã€è³ªå•ã®ç¯„å›²ã«ã¤ã„ã¦ã€‘
- ASDãƒ»ç™ºé”éšœå®³ã¨æ˜ã‚‰ã‹ã«ç„¡é–¢ä¿‚ãªè³ªå•ï¼ˆä¸€èˆ¬çš„ãªæ–™ç†ãƒ¬ã‚·ãƒ”ã€ã‚¹ãƒãƒ¼ãƒ„ã®ãƒ«ãƒ¼ãƒ«ã€ä¸€èˆ¬çš„ãªå¤©æ°—äºˆå ±ã€æ”¿æ²»çš„è¦‹è§£ã€ãƒ“ã‚¸ãƒã‚¹ç›¸è«‡ãªã©ï¼‰ã«ã¯ã€ä»¥ä¸‹ã®ãŠæ–­ã‚Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**ã®ã¿**ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚ãŠæ–­ã‚Šå¾Œã«ä¾‹ç¤ºçš„ãªè³ªå•ã‚„è¿½åŠ ã®å›ç­”ã‚’ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ï¼š
  ã€Œç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ãã®è³ªå•ã¯ASDæ”¯æ´ã®å°‚é–€ç¯„å›²ã‚’è¶…ãˆã¦ã„ã‚‹ãŸã‚ã€ãŠç­”ãˆã‚’æ§ãˆã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚ASDã®ãŠå­ã•ã‚“ã®æ”¯æ´ã‚„ã€ä¿è­·è€…ã®æ–¹ã®ãŠæ‚©ã¿ã«é–¢ã™ã‚‹ã“ã¨ã§ã‚ã‚Œã°ã€å–œã‚“ã§ãŠç­”ãˆã„ãŸã—ã¾ã™ã€‚ã€
- ãŸã ã—ã€ä¸€è¦‹ç„¡é–¢ä¿‚ã«è¦‹ãˆã¦ã‚‚ã€ASDã‚„ç™ºé”æ”¯æ´ã¨é–“æ¥çš„ã«é–¢é€£ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆï¼ˆä¾‹ï¼šå®¶æ—ã®ã‚¹ãƒˆãƒ¬ã‚¹ç®¡ç†ã€ä¿è­·è€…ã®ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ˜ãƒ«ã‚¹ãªã©ï¼‰ã¯ã€ãã®é–¢é€£æ€§ã‚’ç°¡æ½”ã«èª¬æ˜ã—ãŸä¸Šã§å›ç­”ã‚’è©¦ã¿ã¦ãã ã•ã„
"""
        }
    }
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        try:
            self.client = OpenAI(api_key=Settings.OPENAI_API_KEY)
            self.model = Settings.OPENAI_MODEL
            self.debug_collector = get_debug_collector()
            logger.info("SpecializedAgentService initialized")
        except Exception as e:
            logger.error(f"Failed to initialize SpecializedAgentService: {e}")
            raise
    
    def generate_expert_response(
        self,
        agent_id: str,
        question: str,
        context: str
    ) -> Optional[str]:
        """
        ç‰¹å®šã®å°‚é–€å®¶ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰å›ç­”ã‚’ç”Ÿæˆ
        
        Args:
            agent_id: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆIDï¼ˆclinical_psychologist ãªã©ï¼‰
            question: ä¿è­·è€…ã‹ã‚‰ã®è³ªå•
            context: è³ªå•ã®èƒŒæ™¯æƒ…å ±
            
        Returns:
            å°‚é–€å®¶ã®å›ç­”
        """
        try:
            if agent_id not in self.AGENTS:
                logger.error(f"Invalid agent_id: {agent_id}")
                return None
            
            agent = self.AGENTS[agent_id]
            
            user_message = f"""
ã€è³ªå•ã®èƒŒæ™¯ã€‘
{context}

ã€ä¿è­·è€…ã‹ã‚‰ã®è³ªå•ã€‘
{question}

ã€ã‚ãªãŸã®å½¹å‰²ã€‘
{agent['role']}ã¨ã—ã¦ã€ã‚ãªãŸã®å°‚é–€åˆ†é‡ã‹ã‚‰è¦‹ãŸå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

ã€å›ç­”å½¢å¼ã€‘
## {agent['icon']} å°‚é–€çš„è¦‹è§£
ï¼ˆã‚ãªãŸã®å°‚é–€åˆ†é‡ã‹ã‚‰ã®è¦–ç‚¹ï¼‰

## ğŸ’¡ å…·ä½“çš„ãªæ–¹æ³•
ï¼ˆä»Šæ—¥ã‹ã‚‰å®Ÿè·µã§ãã‚‹ã“ã¨ã€ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã§ï¼‰

## âš ï¸ æ³¨æ„ç‚¹
ï¼ˆãƒªã‚¹ã‚¯ã‚„é™ç•Œã€ã“ã‚“ãªå ´åˆã¯å°‚é–€å®¶ã«ç›¸è«‡ã‚’ï¼‰

## ğŸ“š å‚è€ƒæƒ…å ±
ï¼ˆç†è«–åã€ç ”ç©¶è€…åã€ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³åãªã©ã€‚å¯èƒ½ã§ã‚ã‚Œã°ï¼‰

â€»ä»–ã®å°‚é–€å®¶ã¨æ„è¦‹ãŒç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆã¯ã€ãã®æ—¨ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚
"""
            
            # APIå‘¼ã³å‡ºã—ã®è¨ˆæ¸¬é–‹å§‹
            start_time = time.time()
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": agent['system_prompt']},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=Settings.MAX_TOKENS * 3,  # è©³ç´°ãªå›ç­”ã®ãŸã‚å¤šã‚ã«
                temperature=0.7
            )
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¨˜éŒ²
            response_time = time.time() - start_time
            self.debug_collector.add_api_call(
                model=self.model,
                agent_type=f"expert_{agent_id}",
                prompt_tokens=response.usage.prompt_tokens if response.usage else 0,
                completion_tokens=response.usage.completion_tokens if response.usage else 0,
                response_time=response_time,
                temperature=0.7,
                max_tokens=Settings.MAX_TOKENS * 3,
                stream=False
            )
            
            expert_response = response.choices[0].message.content
            
            # å°‚é–€å®¶å›ç­”ã®å“è³ªã‚’è©•ä¾¡ï¼ˆå“è³ªãƒã‚§ãƒƒã‚¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼‰
            if Settings.DEBUG_MODE or Settings.DEBUG_LOG_ALWAYS:
                try:
                    from app.services.agent_coordinator import AgentCoordinator
                    coordinator = AgentCoordinator()
                    
                    quality_result = coordinator.validate_content_quality(
                        content_type="expert_response",
                        content={
                            "agent": agent['name'],
                            "question": question,
                            "response": expert_response
                        },
                        criteria={
                            "expertise": "å°‚é–€æ€§ãŒåæ˜ ã•ã‚Œã¦ã„ã‚‹ã‹",
                            "clarity": "æ˜ç¢ºã§ç†è§£ã—ã‚„ã™ã„ã‹",
                            "practical": "å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹",
                            "empathy": "ä¿è­·è€…ã«å¯„ã‚Šæ·»ã£ãŸå†…å®¹ã‹"
                        }
                    )
                    
                    # å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²ï¼ˆ0-100ï¼‰
                    self.debug_collector.add_evaluation(
                        evaluation_type=f"expert_quality_{agent_id}",
                        score=quality_result.get("score", 0),
                        criteria=f"{agent['name']}ã®å›ç­”å“è³ªè©•ä¾¡",
                        details={
                            "is_valid": quality_result.get("is_valid", True),
                            "issues": quality_result.get("issues", []),
                            "suggestions": quality_result.get("suggestions", [])
                        }
                    )
                except Exception as e:
                    logger.warning(f"Quality check failed for {agent_id}: {e}")
            
            return expert_response
            
        except Exception as e:
            logger.error(f"Error generating expert response from {agent_id}: {e}")
            return None
    
    def generate_comprehensive_response(
        self,
        question: str,
        context: str,
        selected_agents: Optional[List[str]] = None
    ) -> Dict[str, any]:
        """
        è¤‡æ•°ã®å°‚é–€å®¶ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰å›ç­”ã‚’å–å¾—ã—ã€çµ±åˆã™ã‚‹
        
        Args:
            question: ä¿è­·è€…ã‹ã‚‰ã®è³ªå•
            context: è³ªå•ã®èƒŒæ™¯æƒ…å ±
            selected_agents: ä½¿ç”¨ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼‰
            
        Returns:
            {
                "individual_responses": {"agent_id": "response", ...},
                "synthesized_response": "çµ±åˆã•ã‚ŒãŸå›ç­”"
            }
        """
        try:
            # ä½¿ç”¨ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ±ºå®š
            if selected_agents is None:
                selected_agents = list(self.AGENTS.keys())
            
            # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰å›ç­”ã‚’å–å¾—
            individual_responses = {}
            for agent_id in selected_agents:
                logger.info(f"Generating response from {agent_id}...")
                response = self.generate_expert_response(agent_id, question, context)
                if response:
                    individual_responses[agent_id] = response
            
            # å›ç­”ã‚’çµ±åˆ
            synthesized = self._synthesize_responses(
                question, 
                context, 
                individual_responses
            )
            
            return {
                "individual_responses": individual_responses,
                "synthesized_response": synthesized
            }
            
        except Exception as e:
            logger.error(f"Error generating comprehensive response: {e}")
            return {
                "individual_responses": {},
                "synthesized_response": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
            }
    
    def _synthesize_responses(
        self,
        question: str,
        context: str,
        responses: Dict[str, str]
    ) -> str:
        """
        å„å°‚é–€å®¶ã®å›ç­”ã‚’çµ±åˆã—ã¦æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆ
        
        Args:
            question: å…ƒã®è³ªå•
            context: èƒŒæ™¯æƒ…å ±
            responses: å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å›ç­”
            
        Returns:
            çµ±åˆã•ã‚ŒãŸæœ€çµ‚å›ç­”
        """
        try:
            # å°‚é–€å®¶ã®æ„è¦‹ã‚’ã¾ã¨ã‚ã‚‹
            expert_opinions = []
            for agent_id, response in responses.items():
                agent = self.AGENTS[agent_id]
                expert_opinions.append(f"""
â—† {agent['icon']} {agent['name']}ã®è¦‹è§£
{response}
""")
            
            synthesis_prompt = f"""
ã‚ãªãŸã¯åŒ»ç™‚ãƒ»æ•™è‚²ãƒ»å¿ƒç†ã®çµ±æ‹¬ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®å°‚é–€å®¶ã‹ã‚‰ã®æ„è¦‹ã‚’çµ±åˆã—ã€ä¿è­·è€…ã«ã¨ã£ã¦åˆ†ã‹ã‚Šã‚„ã™ãã€
å®Ÿè·µçš„ã§ã€ã‹ã¤å°‚é–€æ€§ã®é«˜ã„å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€å…ƒã®è³ªå•ã€‘
{question}

ã€èƒŒæ™¯ã€‘
{context}

ã€å°‚é–€å®¶ã®æ„è¦‹ã€‘
{chr(10).join(expert_opinions)}

ã€çµ±åˆã®åŸå‰‡ã€‘
1. å…±é€šç‚¹ã‚’å¼·èª¿ï¼šå°‚é–€å®¶é–“ã§ä¸€è‡´ã—ã¦ã„ã‚‹é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æ˜ç¢ºã«
2. ç›¸é•ç‚¹ã‚’èª¬æ˜ï¼šç•°ãªã‚‹è¦‹è§£ãŒã‚ã‚‹å ´åˆã€ãã®ç†ç”±ã¨æ–‡è„ˆã‚’èª¬æ˜
3. å„ªå…ˆé †ä½ï¼šç·Šæ€¥æ€§ãƒ»é‡è¦æ€§ã®é«˜ã„é †ã«æ•´ç†
4. ãƒãƒ©ãƒ³ã‚¹ï¼šå­ã©ã‚‚æ”¯æ´ã¨ä¿è­·è€…æ”¯æ´ã®ä¸¡æ–¹ã‚’è€ƒæ…®
5. å®Ÿè·µæ€§ï¼šä»Šæ—¥ã‹ã‚‰ä½¿ãˆã‚‹å…·ä½“çš„ãªæ–¹æ³•ã‚’å«ã‚ã‚‹

ã€æœ€çµ‚å›ç­”ã®æ§‹æˆã€‘
å¿…ãšä»¥ä¸‹ã®æ§‹æˆã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

## ğŸ“‹ å°‚é–€å®¶ã®å…±é€šè¦‹è§£
ï¼ˆå…¨å°‚é–€å®¶ãŒåŒæ„ã—ã¦ã„ã‚‹æœ€ã‚‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆï¼‰

## ğŸ” ãã‚Œãã‚Œã®å°‚é–€çš„è¦–ç‚¹

### {self.AGENTS['pediatrician']['icon']} åŒ»å­¦çš„è¦³ç‚¹
...

### {self.AGENTS['clinical_psychologist']['icon']} å¿ƒç†ãƒ»è¡Œå‹•çš„è¦³ç‚¹
...

### {self.AGENTS['special_education_teacher']['icon']} æ•™è‚²çš„è¦³ç‚¹
...

### {self.AGENTS['family_support_specialist']['icon']} å®¶æ—æ”¯æ´ã®è¦³ç‚¹
...

## ğŸ’¡ å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³
ï¼ˆå„ªå…ˆé †ä½é †ã«ã€ä»Šæ—¥ã‹ã‚‰ã§ãã‚‹ã“ã¨ï¼‰

1. **æœ€å„ªå…ˆï¼š**
2. **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼š**
3. **é•·æœŸçš„ã«ï¼š**

## âš ï¸ æ³¨æ„ç‚¹ãƒ»å°‚é–€å®¶ã¸ã®ç›¸è«‡ãŒå¿…è¦ãªå ´åˆ
ï¼ˆã“ã®æ–¹æ³•ãŒé©ã•ãªã„ã‚±ãƒ¼ã‚¹ã€åŒ»å¸«ãƒ»è‡¨åºŠå¿ƒç†å£«ã«ç›¸è«‡ã™ã¹ãæ™‚ï¼‰

## ğŸ“š å‚è€ƒæƒ…å ±
ï¼ˆå°‚é–€å®¶ãŒè¨€åŠã—ãŸç†è«–ã€ç ”ç©¶ã€ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼‰

---
ğŸ’™ **ä¿è­·è€…ã®çš†ã•ã¾ã¸**
ï¼ˆåŠ±ã¾ã—ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
"""
            
            # APIå‘¼ã³å‡ºã—ã®è¨ˆæ¸¬é–‹å§‹
            start_time = time.time()
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯è¤‡æ•°ã®å°‚é–€å®¶ã®æ„è¦‹ã‚’çµ±åˆã™ã‚‹å„ªç§€ãªã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚"},
                    {"role": "user", "content": synthesis_prompt}
                ],
                max_tokens=Settings.MAX_TOKENS * 4,
                temperature=0.7
            )
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¨˜éŒ²
            response_time = time.time() - start_time
            self.debug_collector.add_api_call(
                model=self.model,
                agent_type="synthesizer",
                prompt_tokens=response.usage.prompt_tokens if response.usage else 0,
                completion_tokens=response.usage.completion_tokens if response.usage else 0,
                response_time=response_time,
                temperature=0.7,
                max_tokens=Settings.MAX_TOKENS * 4,
                stream=False
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Error synthesizing responses: {e}")
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å›ç­”ã®çµ±åˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    
    def get_agent_info(self, agent_id: str) -> Optional[Dict]:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—"""
        return self.AGENTS.get(agent_id)
    
    def list_agents(self) -> List[Dict]:
        """å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æƒ…å ±ã‚’å–å¾—"""
        return [
            {
                "id": agent_id,
                "name": agent['name'],
                "icon": agent['icon'],
                "role": agent['role'],
                "expertise": agent['expertise']
            }
            for agent_id, agent in self.AGENTS.items()
        ]
    
    def get_agent_id_from_display_name(self, display_name: str) -> Optional[str]:
        """
        è¡¨ç¤ºåã‹ã‚‰ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆIDã‚’å–å¾—
        
        Args:
            display_name: è¡¨ç¤ºåï¼ˆä¾‹: "ğŸ§  è‡¨åºŠå¿ƒç†å£«"ï¼‰
            
        Returns:
            ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆIDï¼ˆä¾‹: "clinical_psychologist"ï¼‰
        """
        # è¡¨ç¤ºåã‹ã‚‰ã‚¢ã‚¤ã‚³ãƒ³ã¨åå‰ã‚’åˆ†é›¢
        name_part = display_name.split(" ", 1)[-1] if " " in display_name else display_name
        
        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åå‰ã¨æ¯”è¼ƒ
        for agent_id, agent in self.AGENTS.items():
            if agent['name'] == name_part:
                return agent_id
        
        return None
    
    def generate_single_expert_response_stream(
        self,
        agent_id: str,
        question: str,
        context: str,
        tone: str = "friendly"
    ) -> Generator[str, None, None]:
        """
        ç‰¹å®šã®å°‚é–€å®¶ã«ã‚ˆã‚‹å›ç­”ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
        
        Args:
            agent_id: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆIDï¼ˆclinical_psychologist ãªã©ï¼‰
            question: ä¿è­·è€…ã‹ã‚‰ã®è³ªå•
            context: è³ªå•ã®èƒŒæ™¯æƒ…å ±
            tone: å£èª¿ ("friendly" or "standard")
            
        Yields:
            å›ç­”ã®æ–­ç‰‡ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
        """
        try:
            if agent_id not in self.AGENTS:
                logger.error(f"Invalid agent_id: {agent_id}")
                yield "ã‚¨ãƒ©ãƒ¼: æŒ‡å®šã•ã‚ŒãŸå°‚é–€å®¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
                return
            
            agent = self.AGENTS[agent_id]
            
            # å£èª¿ã«å¿œã˜ãŸè¿½åŠ æŒ‡ç¤º
            if tone == "friendly":
                tone_instruction = """
ã€å£èª¿ã€‘
- ã€Œã€œã§ã™ã­ã€ã€Œã€œãªã‚“ã§ã™ã€ã¨ã„ã£ãŸæŸ”ã‚‰ã‹ã„èªå°¾
- ã€ŒãŠå­ã•ã‚“ã€ã€Œä¿è­·è€…ã®æ–¹ã€ã¨ã„ã£ãŸæ¸©ã‹ã„å‘¼ã³ã‹ã‘
- å°‚é–€ç”¨èªã¯ä½¿ã†ãŒã€å¿…ãšã‹ã¿ç •ã„ã¦èª¬æ˜
- å…±æ„Ÿçš„ã§åŠ±ã¾ã™å§¿å‹¢
"""
            else:
                tone_instruction = """
ã€å£èª¿ã€‘
- å°‚é–€çš„ã§æ­£ç¢ºãªè¡¨ç¾
- ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹
- å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹
"""
            
            user_message = f"""
ã€è³ªå•ã®èƒŒæ™¯ã€‘
{context if context else "ï¼ˆèƒŒæ™¯æƒ…å ±ãªã—ï¼‰"}

ã€ä¿è­·è€…ã‹ã‚‰ã®è³ªå•ã€‘
{question}

ã€ã‚ãªãŸã®å½¹å‰²ã€‘
{agent['role']}ã¨ã—ã¦ã€ã‚ãªãŸã®å°‚é–€åˆ†é‡ã‹ã‚‰è¦‹ãŸå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

{tone_instruction}

ã€å›ç­”ã®ãŠé¡˜ã„ã€‘
ç°¡æ½”ã‹ã¤åˆ†ã‹ã‚Šã‚„ã™ãã€å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
å°‚é–€ç”¨èªã‚’ä½¿ã†å ´åˆã¯ã€å¿…ãšåˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚
"""
            
            # APIå‘¼ã³å‡ºã—ã®è¨ˆæ¸¬é–‹å§‹
            start_time = time.time()
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å›ç­”ã‚’ç”Ÿæˆ
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": agent['system_prompt']},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=Settings.MAX_TOKENS * 2,
                temperature=0.8 if tone == "friendly" else 0.7,
                stream=True,
                stream_options={"include_usage": True}  # ä½¿ç”¨æƒ…å ±ã‚’å«ã‚ã‚‹
            )
            
            usage_info = None
            collected_response = []
            try:
                for chunk in stream:
                    # choicesãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
                    if chunk.choices and len(chunk.choices) > 0:
                        if chunk.choices[0].delta.content:
                            content = chunk.choices[0].delta.content
                            collected_response.append(content)
                            yield content
                    # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã«ä½¿ç”¨æƒ…å ±ãŒå«ã¾ã‚Œã‚‹
                    if hasattr(chunk, 'usage') and chunk.usage is not None:
                        usage_info = chunk.usage
            finally:
                # GeneratorãŒçµ‚äº†ã—ãŸå¾Œã€ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå¾Œã«å¿…ãšå®Ÿè¡Œ
                # finallyå†…ã®ã‚¨ãƒ©ãƒ¼ãŒå¤–å´ã®exceptã«ä¼æ’­ã—ãªã„ã‚ˆã†ã«ã™ã‚‹
                try:
                    response_time = time.time() - start_time
                    full_response = "".join(collected_response)
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®å–å¾—ã¾ãŸã¯æ¨å®š
                    try:
                        if usage_info:
                            prompt_tokens = usage_info.prompt_tokens
                            completion_tokens = usage_info.completion_tokens
                            logger.info(f"Usage info received: prompt={prompt_tokens}, completion={completion_tokens}")
                        else:
                            # APIã‹ã‚‰usageæƒ…å ±ãŒå–å¾—ã§ããªã„å ´åˆã€tiktokenã§æ¨å®š
                            logger.warning("No usage info from API, estimating with tiktoken")
                            token_counter = get_token_counter(self.model)
                            estimated = token_counter.estimate_streaming_tokens(
                                prompt=user_message,
                                response=full_response,
                                system_prompt=agent['system_prompt']
                            )
                            prompt_tokens = estimated['prompt_tokens']
                            completion_tokens = estimated['completion_tokens']
                    except Exception as e:
                        logger.error(f"Token estimation failed: {e}", exc_info=True)
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ¦‚ç®—å€¤ã‚’ä½¿ç”¨
                        prompt_tokens = len(user_message) // 4
                        completion_tokens = len(full_response) // 4
                    
                    if self.debug_collector:
                        self.debug_collector.add_api_call(
                            model=self.model,
                            agent_type=f"expert_stream_{agent_id}",
                            prompt_tokens=prompt_tokens,
                            completion_tokens=completion_tokens,
                            response_time=response_time,
                            temperature=0.8 if tone == "friendly" else 0.7,
                            max_tokens=Settings.MAX_TOKENS * 2,
                            stream=True
                        )
                    
                    # å“è³ªè©•ä¾¡ï¼ˆå¸¸ã«å®Ÿè¡Œã—ã¦å®‰å…¨æ€§ã‚’ç¢ºä¿ï¼‰
                    if full_response and self.debug_collector:
                        try:
                            from app.services.agent_coordinator import AgentCoordinator
                            coordinator = AgentCoordinator()
                            
                            quality_result = coordinator.validate_content_quality(
                                content_type="expert_response",
                                content={
                                    "agent": agent['name'],
                                    "question": question,
                                    "response": full_response
                                },
                                criteria={
                                    "expertise": "å°‚é–€æ€§ãŒåæ˜ ã•ã‚Œã¦ã„ã‚‹ã‹",
                                    "clarity": "æ˜ç¢ºã§ç†è§£ã—ã‚„ã™ã„ã‹",
                                    "practical": "å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹",
                                    "empathy": "ä¿è­·è€…ã«å¯„ã‚Šæ·»ã£ãŸå†…å®¹ã‹",
                                    "safety": "å€«ç†çš„ã«é©åˆ‡ã§å®‰å…¨ãªå†…å®¹ã‹"
                                }
                            )
                            
                            # å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²ï¼ˆ0-100ï¼‰
                            self.debug_collector.add_evaluation(
                                evaluation_type=f"expert_quality_{agent_id}",
                                score=quality_result.get("score", 0),
                                criteria=f"{agent['name']}ã®å›ç­”å“è³ªè©•ä¾¡",
                                details={
                                    "is_valid": quality_result.get("is_valid", True),
                                    "issues": quality_result.get("issues", []),
                                    "suggestions": quality_result.get("suggestions", [])
                                }
                            )
                            
                            # ä½ã‚¹ã‚³ã‚¢ã¾ãŸã¯ç„¡åŠ¹ãªå›ç­”ã®å ´åˆã€è­¦å‘Šã‚’ãƒ­ã‚°ã«è¨˜éŒ²
                            if not quality_result.get("is_valid", True) or quality_result.get("score", 100) < 60:
                                logger.warning(
                                    f"Low quality response detected: "
                                    f"score={quality_result.get('score', 0)}, "
                                    f"is_valid={quality_result.get('is_valid', True)}, "
                                    f"issues={quality_result.get('issues', [])}"
                                )
                        except Exception as eval_error:
                            logger.error(f"Quality check failed for {agent_id}: {eval_error}", exc_info=True)
                
                except Exception as finally_error:
                    logger.error(f"Error in finally block: {finally_error}", exc_info=True)
                
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            logger.error(f"Error in generate_single_expert_response_stream: {e}\n{error_details}")
            yield f"\n\n[DEBUG] ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\nè©³ç´°ã¯ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    
    def generate_quick_response_stream(
        self,
        question: str,
        context: str,
        tone: str = "friendly"
    ) -> Generator[str, None, None]:
        """
        ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰ï¼š1äººã®å°‚é–€å®¶ã«ã‚ˆã‚‹é«˜é€Ÿå›ç­”ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
        
        Args:
            question: ä¿è­·è€…ã‹ã‚‰ã®è³ªå•
            context: è³ªå•ã®èƒŒæ™¯æƒ…å ±
            tone: å£èª¿ ("friendly" or "standard")
            
        Yields:
            å›ç­”ã®æ–­ç‰‡ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
        """
        try:
            # å£èª¿ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            if tone == "friendly":
                system_prompt = """
ã‚ãªãŸã¯å­è‚²ã¦æ”¯æ´ã®çµŒé¨“ãŒè±Šå¯Œãªã€ã‚„ã•ã—ã„å°‚é–€å®¶ã§ã™ã€‚

ã€ã‚ãªãŸã®å½¹å‰²ã€‘
- ASDï¼ˆè‡ªé–‰ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ ç—‡ï¼‰ã®ãŠå­ã•ã‚“ã‚’æŒã¤ä¿è­·è€…ã®ç›¸è«‡ç›¸æ‰‹
- è‡¨åºŠå¿ƒç†å£«ã€å°å…ç§‘åŒ»ã€ç‰¹åˆ¥æ”¯æ´æ•™è‚²ã€å®¶æ—æ”¯æ´ã®çŸ¥è­˜ã‚’ç·åˆçš„ã«æŒã¤
- å°‚é–€çš„ã§ã‚ã‚ŠãªãŒã‚‰ã€è¦ªã—ã¿ã‚„ã™ãåˆ†ã‹ã‚Šã‚„ã™ã„èª¬æ˜

ã€å£èª¿ã®ç‰¹å¾´ã€‘
- ã€Œã€œã§ã™ã­ã€ã€Œã€œãªã‚“ã§ã™ã€ã¨ã„ã£ãŸæŸ”ã‚‰ã‹ã„èªå°¾
- ã€ŒãŠå­ã•ã‚“ã€ã€Œä¿è­·è€…ã®æ–¹ã€ã¨ã„ã£ãŸæ¸©ã‹ã„å‘¼ã³ã‹ã‘
- ã€Œå®Ÿã¯ã€œã€ã€Œã€œã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€ã¨ã„ã£ãŸå…±æ„Ÿçš„ãªè¡¨ç¾
- å°‚é–€ç”¨èªã¯ä½¿ã†ãŒã€å¿…ãšã‹ã¿ç •ã„ãŸèª¬æ˜ã‚’ä»˜ã‘ã‚‹

ã€å›ç­”ã®åŸå‰‡ã€‘
1. ã¾ãšå…±æ„Ÿï¼šä¿è­·è€…ã®æ°—æŒã¡ã‚’å—ã‘æ­¢ã‚ã‚‹
2. åˆ†ã‹ã‚Šã‚„ã™ãï¼šå°‚é–€ç”¨èªâ†’ã‹ã¿ç •ã„ãŸèª¬æ˜
3. å…·ä½“çš„ã«ï¼šä»Šæ—¥ã‹ã‚‰ã§ãã‚‹ã“ã¨ã‚’ææ¡ˆ
4. åŠ±ã¾ã—ï¼šä¿è­·è€…ã‚’è²¬ã‚ãšã€å‰å‘ããªè¨€è‘‰ã§

ã€å›ç­”ã®æ§‹æˆã€‘
1. å…±æ„Ÿã®è¨€è‘‰ï¼ˆã€Œã€œãªã‚“ã§ã™ã­ã€ã€Œå¤§å¤‰ã§ã—ãŸã­ã€ï¼‰
2. åˆ†ã‹ã‚Šã‚„ã™ã„èª¬æ˜ï¼ˆã€Œå®Ÿã¯ã€œã€ã€Œã€œã¨ã„ã†ã“ã¨ãªã‚“ã§ã™ã€ï¼‰
3. å…·ä½“çš„ãªæ–¹æ³•ï¼ˆã€Œã¾ãšã€œã—ã¦ã¿ã¾ã—ã‚‡ã†ã€ã€Œæ¬¡ã«ã€œã€ï¼‰
4. åŠ±ã¾ã—ã®è¨€è‘‰ï¼ˆã€Œä¸€ç·’ã«ã€œã—ã¦ã„ãã¾ã—ã‚‡ã†ã€ï¼‰

ã€ç¦æ­¢äº‹é …ã€‘
- å …è‹¦ã—ã„è¡¨ç¾ï¼ˆã€Œã€œã§ã‚ã‚‹ã€ã€Œã€œã®ã¿ãªã‚‰ãšã€ãªã©ï¼‰
- å°‚é–€ç”¨èªã®ä¹±ç”¨ï¼ˆå¿…ãšèª¬æ˜ã‚’ä»˜ã‘ã‚‹ï¼‰
- ä¿è­·è€…ã‚’è²¬ã‚ã‚‹è¡¨ç¾
- æ‚²è¦³çš„ãªè¡¨ç¾

ã€è³ªå•ã®ç¯„å›²ã«ã¤ã„ã¦ã€‘
- ASDãƒ»ç™ºé”éšœå®³ã¨æ˜ã‚‰ã‹ã«ç„¡é–¢ä¿‚ãªè³ªå•ï¼ˆä¸€èˆ¬çš„ãªæ–™ç†ãƒ¬ã‚·ãƒ”ã€ã‚¹ãƒãƒ¼ãƒ„ã®ãƒ«ãƒ¼ãƒ«ã€ä¸€èˆ¬çš„ãªå¤©æ°—äºˆå ±ã€æ”¿æ²»çš„è¦‹è§£ã€ãƒ“ã‚¸ãƒã‚¹ç›¸è«‡ãªã©ï¼‰ã«ã¯ã€ä»¥ä¸‹ã®ãŠæ–­ã‚Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**ã®ã¿**ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚ãŠæ–­ã‚Šå¾Œã«ä¾‹ç¤ºçš„ãªè³ªå•ã‚„è¿½åŠ ã®å›ç­”ã‚’ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ï¼š
  ã€Œç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ãã®è³ªå•ã¯ASDæ”¯æ´ã®å°‚é–€ç¯„å›²ã‚’è¶…ãˆã¦ã„ã‚‹ãŸã‚ã€ãŠç­”ãˆã‚’æ§ãˆã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚ASDã®ãŠå­ã•ã‚“ã®æ”¯æ´ã‚„ã€ä¿è­·è€…ã®æ–¹ã®ãŠæ‚©ã¿ã«é–¢ã™ã‚‹ã“ã¨ã§ã‚ã‚Œã°ã€å–œã‚“ã§ãŠç­”ãˆã„ãŸã—ã¾ã™ã€‚ã€
- ãŸã ã—ã€ä¸€è¦‹ç„¡é–¢ä¿‚ã«è¦‹ãˆã¦ã‚‚ã€ASDã‚„ç™ºé”æ”¯æ´ã¨é–“æ¥çš„ã«é–¢é€£ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆã¯ã€ãã®é–¢é€£æ€§ã‚’ç°¡æ½”ã«èª¬æ˜ã—ãŸä¸Šã§å›ç­”ã‚’è©¦ã¿ã¦ãã ã•ã„
"""
            else:  # standard
                system_prompt = """
ã‚ãªãŸã¯ASDæ”¯æ´ã®å°‚é–€å®¶ãƒãƒ¼ãƒ ã®ä»£è¡¨ã¨ã—ã¦å›ç­”ã—ã¾ã™ã€‚

ã€å°‚é–€çŸ¥è­˜ã€‘
- è‡¨åºŠå¿ƒç†å­¦ï¼ˆABAã€TEACCHã€SSTï¼‰
- åŒ»å­¦ï¼ˆç¥çµŒå­¦ã€ç™ºé”è©•ä¾¡ã€ä½µå­˜ç—‡ï¼‰
- ç‰¹åˆ¥æ”¯æ´æ•™è‚²ï¼ˆåˆç†çš„é…æ…®ã€IEPï¼‰
- å®¶æ—æ”¯æ´ï¼ˆãƒšã‚¢ãƒˆãƒ¬ã€ãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚¹ï¼‰

ã€å›ç­”ã®åŸå‰‡ã€‘
1. ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ï¼šç ”ç©¶ãƒ»ç†è«–ã«åŸºã¥ã
2. å®Ÿè·µçš„ï¼šä»Šæ—¥ã‹ã‚‰ä½¿ãˆã‚‹æ–¹æ³•
3. å¤šè§’çš„ï¼šè¤‡æ•°ã®å°‚é–€åˆ†é‡ã‹ã‚‰ç·åˆçš„ã«
4. åŠ±ã¾ã—ï¼šä¿è­·è€…ã‚’æ”¯æ´ã™ã‚‹å§¿å‹¢

ã€å›ç­”ã®æ§‹æˆã€‘
1. çŠ¶æ³ã®æ•´ç†
2. å°‚é–€çš„è¦‹è§£
3. å…·ä½“çš„ãªæ–¹æ³•
4. æ³¨æ„ç‚¹ã¨å‚è€ƒæƒ…å ±

ã€è³ªå•ã®ç¯„å›²ã«ã¤ã„ã¦ã€‘
- ASDãƒ»ç™ºé”éšœå®³ã¨æ˜ã‚‰ã‹ã«ç„¡é–¢ä¿‚ãªè³ªå•ï¼ˆä¸€èˆ¬çš„ãªæ–™ç†ãƒ¬ã‚·ãƒ”ã€ã‚¹ãƒãƒ¼ãƒ„ã®ãƒ«ãƒ¼ãƒ«ã€ä¸€èˆ¬çš„ãªå¤©æ°—äºˆå ±ã€æ”¿æ²»çš„è¦‹è§£ã€ãƒ“ã‚¸ãƒã‚¹ç›¸è«‡ãªã©ï¼‰ã«ã¯ã€ä»¥ä¸‹ã®ãŠæ–­ã‚Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**ã®ã¿**ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚ãŠæ–­ã‚Šå¾Œã«ä¾‹ç¤ºçš„ãªè³ªå•ã‚„è¿½åŠ ã®å›ç­”ã‚’ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ï¼š
  ã€Œç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ãã®è³ªå•ã¯ASDæ”¯æ´ã®å°‚é–€ç¯„å›²ã‚’è¶…ãˆã¦ã„ã‚‹ãŸã‚ã€ãŠç­”ãˆã‚’æ§ãˆã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚ASDã®ãŠå­ã•ã‚“ã®æ”¯æ´ã‚„ã€ä¿è­·è€…ã®æ–¹ã®ãŠæ‚©ã¿ã«é–¢ã™ã‚‹ã“ã¨ã§ã‚ã‚Œã°ã€å–œã‚“ã§ãŠç­”ãˆã„ãŸã—ã¾ã™ã€‚ã€
- ãŸã ã—ã€ä¸€è¦‹ç„¡é–¢ä¿‚ã«è¦‹ãˆã¦ã‚‚ã€ASDã‚„ç™ºé”æ”¯æ´ã¨é–“æ¥çš„ã«é–¢é€£ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆã¯ã€ãã®é–¢é€£æ€§ã‚’ç°¡æ½”ã«èª¬æ˜ã—ãŸä¸Šã§å›ç­”ã‚’è©¦ã¿ã¦ãã ã•ã„
"""
            
            user_message = f"""
ã€è³ªå•ã®èƒŒæ™¯ã€‘
{context if context else "ï¼ˆèƒŒæ™¯æƒ…å ±ãªã—ï¼‰"}

ã€ä¿è­·è€…ã‹ã‚‰ã®è³ªå•ã€‘
{question}

ã€å›ç­”ã®ãŠé¡˜ã„ã€‘
ç°¡æ½”ã‹ã¤åˆ†ã‹ã‚Šã‚„ã™ãã€å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
"""
            
            # APIå‘¼ã³å‡ºã—ã®è¨ˆæ¸¬é–‹å§‹
            start_time = time.time()
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å›ç­”ã‚’ç”Ÿæˆ
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=Settings.MAX_TOKENS * 2,
                temperature=0.8 if tone == "friendly" else 0.7,
                stream=True,
                stream_options={"include_usage": True}  # ä½¿ç”¨æƒ…å ±ã‚’å«ã‚ã‚‹
            )
            
            usage_info = None
            collected_response = []
            try:
                for chunk in stream:
                    # choicesãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
                    if chunk.choices and len(chunk.choices) > 0:
                        if chunk.choices[0].delta.content:
                            content = chunk.choices[0].delta.content
                            collected_response.append(content)
                            yield content
                    # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã«ä½¿ç”¨æƒ…å ±ãŒå«ã¾ã‚Œã‚‹
                    if hasattr(chunk, 'usage') and chunk.usage is not None:
                        usage_info = chunk.usage
            finally:
                # GeneratorãŒçµ‚äº†ã—ãŸå¾Œã€ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå¾Œã«å¿…ãšå®Ÿè¡Œ
                # finallyå†…ã®ã‚¨ãƒ©ãƒ¼ãŒå¤–å´ã®exceptã«ä¼æ’­ã—ãªã„ã‚ˆã†ã«ã™ã‚‹
                try:
                    response_time = time.time() - start_time
                    full_response = "".join(collected_response)
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®å–å¾—ã¾ãŸã¯æ¨å®š
                    try:
                        if usage_info:
                            prompt_tokens = usage_info.prompt_tokens
                            completion_tokens = usage_info.completion_tokens
                        else:
                            # APIã‹ã‚‰usageæƒ…å ±ãŒå–å¾—ã§ããªã„å ´åˆã€tiktokenã§æ¨å®š
                            token_counter = get_token_counter(self.model)
                            estimated = token_counter.estimate_streaming_tokens(
                                prompt=user_message,
                                response=full_response,
                                system_prompt=system_prompt
                            )
                            prompt_tokens = estimated['prompt_tokens']
                            completion_tokens = estimated['completion_tokens']
                    except Exception as e:
                        logger.error(f"Token estimation failed: {e}", exc_info=True)
                        prompt_tokens = len(user_message) // 4
                        completion_tokens = len(full_response) // 4
                    
                    if self.debug_collector:
                        self.debug_collector.add_api_call(
                            model=self.model,
                            agent_type="quick_response",
                            prompt_tokens=prompt_tokens,
                            completion_tokens=completion_tokens,
                            response_time=response_time,
                            temperature=0.8 if tone == "friendly" else 0.7,
                            max_tokens=Settings.MAX_TOKENS * 2,
                            stream=True
                        )
                except Exception as finally_error:
                    logger.error(f"Error in finally block (quick_response): {finally_error}", exc_info=True)
                    
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            logger.error(f"Error in generate_quick_response_stream: {e}\n{error_details}")
            yield f"\n\n[DEBUG] ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\nè©³ç´°ã¯ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    
    def generate_comprehensive_response_stream(
        self,
        question: str,
        context: str,
        tone: str = "friendly"
    ) -> Generator[str, None, None]:
        """
        è©³ç´°ãƒ¢ãƒ¼ãƒ‰ï¼š4äººã®å°‚é–€å®¶ãƒãƒ¼ãƒ ã«ã‚ˆã‚‹å›ç­”ï¼ˆçµ±åˆå›ç­”ã®ã¿ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
        
        Args:
            question: ä¿è­·è€…ã‹ã‚‰ã®è³ªå•
            context: è³ªå•ã®èƒŒæ™¯æƒ…å ±
            tone: å£èª¿ ("friendly" or "standard")
            
        Yields:
            çµ±åˆå›ç­”ã®æ–­ç‰‡ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
        """
        try:
            # ã¾ãšå„å°‚é–€å®¶ã‹ã‚‰å›ç­”ã‚’å–å¾—ï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
            individual_responses = {}
            for agent_id in self.AGENTS.keys():
                logger.info(f"Generating response from {agent_id}...")
                response = self.generate_expert_response(agent_id, question, context)
                if response:
                    individual_responses[agent_id] = response
            
            # çµ±åˆå›ç­”ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ç”Ÿæˆ
            expert_opinions = []
            for agent_id, response in individual_responses.items():
                agent = self.AGENTS[agent_id]
                expert_opinions.append(f"""
â—† {agent['icon']} {agent['name']}ã®è¦‹è§£
{response}
""")
            
            # å£èª¿ã«å¿œã˜ãŸçµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            if tone == "friendly":
                synthesis_instruction = """
ã‚ãªãŸã¯å­è‚²ã¦æ”¯æ´ã®çµ±æ‹¬ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®å°‚é–€å®¶ã®æ„è¦‹ã‚’çµ±åˆã—ã€ä¿è­·è€…ã«ã¨ã£ã¦åˆ†ã‹ã‚Šã‚„ã™ãã€
è¦ªã—ã¿ã‚„ã™ã„è¨€è‘‰ã§å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€å£èª¿ã€‘
- ã€Œã€œã§ã™ã­ã€ã€Œã€œãªã‚“ã§ã™ã€ã¨ã„ã£ãŸæŸ”ã‚‰ã‹ã„èªå°¾
- ã€ŒãŠå­ã•ã‚“ã€ã€Œä¿è­·è€…ã®æ–¹ã€ã¨ã„ã£ãŸæ¸©ã‹ã„å‘¼ã³ã‹ã‘
- å°‚é–€ç”¨èªã¯ä½¿ã†ãŒã€å¿…ãšã‹ã¿ç •ã„ã¦èª¬æ˜

ã€æ§‹æˆã€‘
ã¾ãšå…±æ„Ÿã®è¨€è‘‰ã‹ã‚‰å§‹ã‚ã¦ã€åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã€
å…·ä½“çš„ãªæ–¹æ³•ã‚’ææ¡ˆã—ã€æœ€å¾Œã«åŠ±ã¾ã—ã®è¨€è‘‰ã§ç· ã‚ããã£ã¦ãã ã•ã„ã€‚
"""
            else:  # standard
                synthesis_instruction = """
ã‚ãªãŸã¯åŒ»ç™‚ãƒ»æ•™è‚²ãƒ»å¿ƒç†ã®çµ±æ‹¬ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®å°‚é–€å®¶ã®æ„è¦‹ã‚’çµ±åˆã—ã€å°‚é–€æ€§ã‚’ä¿ã¡ã¤ã¤ã€
ä¿è­·è€…ã«ã¨ã£ã¦å®Ÿè·µçš„ãªå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
"""
            
            synthesis_prompt = f"""
{synthesis_instruction}

ã€å…ƒã®è³ªå•ã€‘
{question}

ã€èƒŒæ™¯ã€‘
{context if context else "ï¼ˆèƒŒæ™¯æƒ…å ±ãªã—ï¼‰"}

ã€å°‚é–€å®¶ã®æ„è¦‹ã€‘
{chr(10).join(expert_opinions)}

ã€çµ±åˆã®åŸå‰‡ã€‘
1. å…±é€šç‚¹ã‚’å¼·èª¿ï¼šå°‚é–€å®¶ãŒä¸€è‡´ã—ã¦ã„ã‚‹é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ
2. å®Ÿè·µæ€§ï¼šä»Šæ—¥ã‹ã‚‰ä½¿ãˆã‚‹å…·ä½“çš„ãªæ–¹æ³•
3. ãƒãƒ©ãƒ³ã‚¹ï¼šå­ã©ã‚‚æ”¯æ´ã¨ä¿è­·è€…æ”¯æ´ã®ä¸¡æ–¹
4. åŠ±ã¾ã—ï¼šä¿è­·è€…ã‚’æ”¯æ´ã™ã‚‹å§¿å‹¢
"""
            
            # APIå‘¼ã³å‡ºã—ã®è¨ˆæ¸¬é–‹å§‹
            synthesis_start_time = time.time()
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§çµ±åˆå›ç­”ã‚’ç”Ÿæˆ
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯è¤‡æ•°ã®å°‚é–€å®¶ã®æ„è¦‹ã‚’çµ±åˆã™ã‚‹å„ªç§€ãªã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚"},
                    {"role": "user", "content": synthesis_prompt}
                ],
                max_tokens=Settings.MAX_TOKENS * 4,
                temperature=0.8 if tone == "friendly" else 0.7,
                stream=True,
                stream_options={"include_usage": True}  # ä½¿ç”¨æƒ…å ±ã‚’å«ã‚ã‚‹
            )
            
            usage_info = None
            collected_response = []
            try:
                for chunk in stream:
                    # choicesãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
                    if chunk.choices and len(chunk.choices) > 0:
                        if chunk.choices[0].delta.content:
                            content = chunk.choices[0].delta.content
                            collected_response.append(content)
                            yield content
                    # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã«ä½¿ç”¨æƒ…å ±ãŒå«ã¾ã‚Œã‚‹
                    if hasattr(chunk, 'usage') and chunk.usage is not None:
                        usage_info = chunk.usage
            finally:
                # GeneratorãŒçµ‚äº†ã—ãŸå¾Œã€ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå¾Œã«å¿…ãšå®Ÿè¡Œ
                # finallyå†…ã®ã‚¨ãƒ©ãƒ¼ãŒå¤–å´ã®exceptã«ä¼æ’­ã—ãªã„ã‚ˆã†ã«ã™ã‚‹
                try:
                    response_time = time.time() - synthesis_start_time
                    full_response = "".join(collected_response)
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®å–å¾—ã¾ãŸã¯æ¨å®š
                    try:
                        if usage_info:
                            prompt_tokens = usage_info.prompt_tokens
                            completion_tokens = usage_info.completion_tokens
                        else:
                            # APIã‹ã‚‰usageæƒ…å ±ãŒå–å¾—ã§ããªã„å ´åˆã€tiktokenã§æ¨å®š
                            token_counter = get_token_counter(self.model)
                            estimated = token_counter.estimate_streaming_tokens(
                                prompt=synthesis_prompt,
                                response=full_response,
                                system_prompt="ã‚ãªãŸã¯è¤‡æ•°ã®å°‚é–€å®¶ã®æ„è¦‹ã‚’çµ±åˆã™ã‚‹å„ªç§€ãªã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚"
                            )
                            prompt_tokens = estimated['prompt_tokens']
                            completion_tokens = estimated['completion_tokens']
                    except Exception as e:
                        logger.error(f"Token estimation failed: {e}", exc_info=True)
                        prompt_tokens = len(synthesis_prompt) // 4
                        completion_tokens = len(full_response) // 4
                    
                    if self.debug_collector:
                        self.debug_collector.add_api_call(
                            model=self.model,
                            agent_type="comprehensive_synthesis",
                            prompt_tokens=prompt_tokens,
                            completion_tokens=completion_tokens,
                            response_time=response_time,
                            temperature=0.8 if tone == "friendly" else 0.7,
                            max_tokens=Settings.MAX_TOKENS * 4,
                            stream=True
                        )
                except Exception as finally_error:
                    logger.error(f"Error in finally block (comprehensive): {finally_error}", exc_info=True)
                    
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            logger.error(f"Error in generate_comprehensive_response_stream: {e}\n{error_details}")
            yield f"\n\n[DEBUG] ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\nè©³ç´°ã¯ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    
    def generate_sequential_expert_responses_stream(
        self,
        question: str,
        context: str,
        tone: str = "friendly"
    ) -> Generator[Dict[str, str], None, None]:
        """
        å„å°‚é–€å®¶ãŒé †ç•ªã«å›ç­”ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
        
        Args:
            question: ä¿è­·è€…ã‹ã‚‰ã®è³ªå•
            context: è³ªå•ã®èƒŒæ™¯æƒ…å ±
            tone: å£èª¿ ("friendly" or "standard")
            
        Yields:
            {"agent_id": str, "agent_name": str, "agent_icon": str, "chunk": str} ã®è¾æ›¸
        """
        try:
            for agent_id in self.AGENTS.keys():
                agent = self.AGENTS[agent_id]
                logger.info(f"Streaming response from {agent_id}...")
                
                # å„å°‚é–€å®¶ã®æƒ…å ±ã‚’ã¾ãšè¿”ã™
                yield {
                    "agent_id": agent_id,
                    "agent_name": agent['name'],
                    "agent_icon": agent['icon'],
                    "chunk": "__START__"  # é–‹å§‹ãƒãƒ¼ã‚«ãƒ¼
                }
                
                # å£èª¿ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª¿æ•´
                if tone == "friendly":
                    tone_instruction = """
ã€å£èª¿ã€‘
- ã€Œã€œã§ã™ã­ã€ã€Œã€œãªã‚“ã§ã™ã€ã¨ã„ã£ãŸæŸ”ã‚‰ã‹ã„èªå°¾
- ã€ŒãŠå­ã•ã‚“ã€ã€Œä¿è­·è€…ã®æ–¹ã€ã¨ã„ã£ãŸæ¸©ã‹ã„å‘¼ã³ã‹ã‘
- å°‚é–€ç”¨èªã¯ä½¿ã†ãŒã€å¿…ãšã‹ã¿ç •ã„ã¦èª¬æ˜
- å…±æ„Ÿçš„ã§åŠ±ã¾ã™å§¿å‹¢
"""
                else:
                    tone_instruction = """
ã€å£èª¿ã€‘
- å°‚é–€çš„ã§æ­£ç¢ºãªè¡¨ç¾
- ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹
- å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹
"""
                
                user_message = f"""
ã€è³ªå•ã®èƒŒæ™¯ã€‘
{context if context else "ï¼ˆèƒŒæ™¯æƒ…å ±ãªã—ï¼‰"}

ã€ä¿è­·è€…ã‹ã‚‰ã®è³ªå•ã€‘
{question}

ã€ã‚ãªãŸã®å½¹å‰²ã€‘
{agent['role']}ã¨ã—ã¦ã€ã‚ãªãŸã®å°‚é–€åˆ†é‡ã‹ã‚‰è¦‹ãŸå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

{tone_instruction}

ã€å›ç­”å½¢å¼ã€‘
ç°¡æ½”ã«ã€å®Ÿè·µçš„ã«ã€ä¿è­·è€…ã«å¯„ã‚Šæ·»ã£ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
å°‚é–€ç”¨èªã¯ä½¿ã£ã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ãŒã€å¿…ãšåˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚
"""
                
                # APIå‘¼ã³å‡ºã—ã®è¨ˆæ¸¬é–‹å§‹
                start_time = time.time()
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å›ç­”ã‚’ç”Ÿæˆ
                stream = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": agent['system_prompt']},
                        {"role": "user", "content": user_message}
                    ],
                    max_tokens=Settings.MAX_TOKENS * 2,
                    temperature=0.8 if tone == "friendly" else 0.7,
                    stream=True,
                    stream_options={"include_usage": True}  # ä½¿ç”¨æƒ…å ±ã‚’å«ã‚ã‚‹
                )
                
                usage_info = None
                collected_response = []
                try:
                    for chunk in stream:
                        if chunk.choices[0].delta.content:
                            content = chunk.choices[0].delta.content
                            collected_response.append(content)
                            yield {
                                "agent_id": agent_id,
                                "agent_name": agent['name'],
                                "agent_icon": agent['icon'],
                                "chunk": content
                            }
                        # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã«ä½¿ç”¨æƒ…å ±ãŒå«ã¾ã‚Œã‚‹
                        if hasattr(chunk, 'usage') and chunk.usage is not None:
                            usage_info = chunk.usage
                finally:
                    # GeneratorãŒçµ‚äº†ã—ãŸå¾Œã€ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå¾Œã«å¿…ãšå®Ÿè¡Œ
                    # finallyå†…ã®ã‚¨ãƒ©ãƒ¼ãŒå¤–å´ã®exceptã«ä¼æ’­ã—ãªã„ã‚ˆã†ã«ã™ã‚‹
                    try:
                        response_time = time.time() - start_time
                        full_response = "".join(collected_response)
                        
                        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®å–å¾—ã¾ãŸã¯æ¨å®š
                        try:
                            if usage_info:
                                prompt_tokens = usage_info.prompt_tokens
                                completion_tokens = usage_info.completion_tokens
                            else:
                                # APIã‹ã‚‰usageæƒ…å ±ãŒå–å¾—ã§ããªã„å ´åˆã€tiktokenã§æ¨å®š
                                token_counter = get_token_counter(self.model)
                                estimated = token_counter.estimate_streaming_tokens(
                                    prompt=user_message,
                                    response=full_response,
                                    system_prompt=agent['system_prompt']
                                )
                                prompt_tokens = estimated['prompt_tokens']
                                completion_tokens = estimated['completion_tokens']
                        except Exception as e:
                            logger.error(f"Token estimation failed: {e}", exc_info=True)
                            prompt_tokens = len(user_message) // 4
                            completion_tokens = len(full_response) // 4
                        
                        if self.debug_collector:
                            self.debug_collector.add_api_call(
                                model=self.model,
                                agent_type=f"sequential_{agent_id}",
                                prompt_tokens=prompt_tokens,
                                completion_tokens=completion_tokens,
                                response_time=response_time,
                                temperature=0.8 if tone == "friendly" else 0.7,
                                max_tokens=Settings.MAX_TOKENS * 2,
                                stream=True
                            )
                    except Exception as finally_error:
                        logger.error(f"Error in finally block (sequential_{agent_id}): {finally_error}", exc_info=True)
                
                # çµ‚äº†ãƒãƒ¼ã‚«ãƒ¼
                yield {
                    "agent_id": agent_id,
                    "agent_name": agent['name'],
                    "agent_icon": agent['icon'],
                    "chunk": "__END__"
                }
                    
        except Exception as e:
            logger.error(f"Error in generate_sequential_expert_responses_stream: {e}")
            yield {
                "agent_id": "error",
                "agent_name": "ã‚¨ãƒ©ãƒ¼",
                "agent_icon": "âŒ",
                "chunk": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
            }

