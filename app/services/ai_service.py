"""
AI service for OpenAI API interactions.
"""

from openai import OpenAI
from typing import Optional, Generator
import logging
import time

from app.config.settings import Settings
from app.config.prompts import AI_PERSONAS, get_feedback_system_prompt, GUIDE_SYSTEM_PROMPT
from app.utils.error_handler import ErrorHandler
from app.utils.debug_info import get_debug_collector

logger = logging.getLogger(__name__)


class AIService:
    """OpenAI APIã¨ã®ã‚„ã‚Šå–ã‚Šã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """AIServiceã®åˆæœŸåŒ–"""
        try:
            self.client = OpenAI(api_key=Settings.OPENAI_API_KEY)
            self.model = Settings.OPENAI_MODEL
            self.debug_collector = get_debug_collector()
            logger.info("AIService initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize AIService: {e}")
            raise
    
    def generate_feedback(self, scene_text: str, selected_choice: str, 
                         evaluation: str, hint: str = "") -> Optional[str]:
        """
        å­ã©ã‚‚ã®è¡Œå‹•é¸æŠã«å¯¾ã™ã‚‹AIãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç”Ÿæˆ
        
        Args:
            scene_text: ã‚·ãƒ¼ãƒ³ã®èª¬æ˜
            selected_choice: é¸æŠã•ã‚ŒãŸè¡Œå‹•
            evaluation: è©•ä¾¡ï¼ˆappropriate/acceptable/inappropriateï¼‰
            hint: AIåˆ¤å®šã®ãƒ’ãƒ³ãƒˆ
            
        Returns:
            AIãŒç”Ÿæˆã—ãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ–‡å­—åˆ—ã€‚å¤±æ•—æ™‚ã¯None
        """
        try:
            system_prompt = get_feedback_system_prompt(
                scene_text=scene_text,
                selected_choice=selected_choice,
                evaluation=evaluation,
                hint=hint
            )
            
            # APIå‘¼ã³å‡ºã—ã®è¨ˆæ¸¬é–‹å§‹
            start_time = time.time()
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"é¸æŠã—ãŸè¡Œå‹•: {selected_choice}"}
                ],
                max_tokens=Settings.MAX_TOKENS,
                temperature=Settings.TEMPERATURE
            )
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¨˜éŒ²
            response_time = time.time() - start_time
            self.debug_collector.add_api_call(
                model=self.model,
                agent_type="feedback_generator",
                prompt_tokens=response.usage.prompt_tokens if response.usage else 0,
                completion_tokens=response.usage.completion_tokens if response.usage else 0,
                response_time=response_time,
                temperature=Settings.TEMPERATURE,
                max_tokens=Settings.MAX_TOKENS,
                stream=False
            )
            
            feedback = response.choices[0].message.content
            
            # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®å“è³ªã‚’è©•ä¾¡ï¼ˆå“è³ªãƒã‚§ãƒƒã‚¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼‰
            if Settings.DEBUG_MODE or Settings.DEBUG_LOG_ALWAYS:
                try:
                    from app.services.agent_coordinator import AgentCoordinator
                    coordinator = AgentCoordinator()
                    
                    quality_result = coordinator.validate_content_quality(
                        content_type="feedback",
                        content={
                            "feedback": feedback,
                            "evaluation": evaluation,
                            "choice": selected_choice
                        },
                        criteria={
                            "clarity": "ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãŒæ˜ç¢ºã§ç†è§£ã—ã‚„ã™ã„ã‹",
                            "appropriateness": "è©•ä¾¡ã«é©ã—ãŸå†…å®¹ã‹",
                            "educational_value": "æ•™è‚²çš„ä¾¡å€¤ãŒã‚ã‚‹ã‹"
                        }
                    )
                    
                    # å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²ï¼ˆ0-100ï¼‰
                    self.debug_collector.add_evaluation(
                        evaluation_type="feedback_quality",
                        score=quality_result.get("score", 0),
                        criteria="å“è³ªç®¡ç†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹è©•ä¾¡",
                        details={
                            "is_valid": quality_result.get("is_valid", True),
                            "issues": quality_result.get("issues", []),
                            "suggestions": quality_result.get("suggestions", []),
                            "user_evaluation": evaluation
                        }
                    )
                except Exception as e:
                    logger.warning(f"Quality check failed: {e}")
            logger.info(f"Generated feedback for choice: {selected_choice}")
            return feedback
            
        except Exception as e:
            logger.error(f"Error generating feedback: {e}")
            ErrorHandler.handle_api_error(e)
            return None
    
    def generate_feedback_stream(self, scene_text: str, selected_choice: str, 
                                 evaluation: str, hint: str = "") -> Generator[str, None, None]:
        """
        å­ã©ã‚‚ã®è¡Œå‹•é¸æŠã«å¯¾ã™ã‚‹AIãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ
        
        Args:
            scene_text: ã‚·ãƒ¼ãƒ³ã®èª¬æ˜
            selected_choice: é¸æŠã•ã‚ŒãŸè¡Œå‹•
            evaluation: è©•ä¾¡
            hint: AIåˆ¤å®šã®ãƒ’ãƒ³ãƒˆ
            
        Yields:
            AIãŒç”Ÿæˆã—ãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®æ–­ç‰‡
        """
        try:
            system_prompt = get_feedback_system_prompt(
                scene_text=scene_text,
                selected_choice=selected_choice,
                evaluation=evaluation,
                hint=hint
            )
            
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"é¸æŠã—ãŸè¡Œå‹•: {selected_choice}"}
                ],
                max_tokens=Settings.MAX_TOKENS,
                temperature=Settings.TEMPERATURE,
                stream=True
            )
            
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
            
            logger.info(f"Generated streaming feedback for choice: {selected_choice}")
            
        except Exception as e:
            logger.error(f"Error generating streaming feedback: {e}")
            ErrorHandler.handle_api_error(e)
            yield ""
    
    def answer_parent_question(self, question: str, ai_mode: str) -> Optional[str]:
        """
        ä¿è­·è€…ã‹ã‚‰ã®è³ªå•ã«å¯¾ã—ã¦AIãŒå›ç­”ã‚’ç”Ÿæˆ
        
        Args:
            question: ä¿è­·è€…ã‹ã‚‰ã®è³ªå•
            ai_mode: AIäººæ ¼ãƒ¢ãƒ¼ãƒ‰
            
        Returns:
            AIãŒç”Ÿæˆã—ãŸå›ç­”æ–‡å­—åˆ—ã€‚å¤±æ•—æ™‚ã¯None
        """
        try:
            if ai_mode not in AI_PERSONAS:
                logger.error(f"Invalid AI mode: {ai_mode}")
                return None
            
            system_prompt = AI_PERSONAS[ai_mode]
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": question}
                ],
                max_tokens=Settings.MAX_TOKENS * 2,  # ä¿è­·è€…å‘ã‘ã¯å°‘ã—é•·ã‚ã«
                temperature=Settings.TEMPERATURE
            )
            
            answer = response.choices[0].message.content
            logger.info(f"Generated answer for question in mode: {ai_mode}")
            return answer
            
        except Exception as e:
            logger.error(f"Error answering parent question: {e}")
            ErrorHandler.handle_api_error(e)
            return None
    
    def answer_parent_question_stream(self, question: str, 
                                     ai_mode: str) -> Generator[str, None, None]:
        """
        ä¿è­·è€…ã‹ã‚‰ã®è³ªå•ã«å¯¾ã—ã¦AIãŒå›ç­”ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ
        
        Args:
            question: ä¿è­·è€…ã‹ã‚‰ã®è³ªå•
            ai_mode: AIäººæ ¼ãƒ¢ãƒ¼ãƒ‰
            
        Yields:
            AIãŒç”Ÿæˆã—ãŸå›ç­”ã®æ–­ç‰‡
        """
        try:
            if ai_mode not in AI_PERSONAS:
                logger.error(f"Invalid AI mode: {ai_mode}")
                yield ""
                return
            
            system_prompt = AI_PERSONAS[ai_mode]
            
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": question}
                ],
                max_tokens=Settings.MAX_TOKENS * 2,
                temperature=Settings.TEMPERATURE,
                stream=True
            )
            
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
            
            logger.info(f"Generated streaming answer for question in mode: {ai_mode}")
            
        except Exception as e:
            logger.error(f"Error answering parent question (stream): {e}")
            ErrorHandler.handle_api_error(e)
            yield ""

    def get_situation_guide(self, event_name: str, scene_description: str,
                            child_action: str, parent_action: str, ai_mode: str) -> Optional[str]:
        """
        ä¿è­·è€…å‘ã‘ã‚·ãƒãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³åˆ¥ã‚¬ã‚¤ãƒ‰ã‚’ç”Ÿæˆ

        Args:
            event_name: ã‚¤ãƒ™ãƒ³ãƒˆå
            scene_description: ã‚·ãƒ¼ãƒ³ã®èª¬æ˜
            child_action: å­ã©ã‚‚ãŒã¨ã£ãŸè¡Œå‹•
            parent_action: ä¿è­·è€…ãŒé¸æŠã—ãŸè¡Œå‹•
            ai_mode: AIäººæ ¼ãƒ¢ãƒ¼ãƒ‰

        Returns:
            AIãŒç”Ÿæˆã—ãŸã‚¬ã‚¤ãƒ‰æ–‡å­—åˆ—ã€‚å¤±æ•—æ™‚ã¯None
        """
        try:
            if ai_mode not in AI_PERSONAS:
                logger.error(f"Invalid AI mode: {ai_mode}")
                return None

            # è¦ªã®è¡Œå‹•ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‹ã‚‰è©•ä¾¡ã¨ãƒ’ãƒ³ãƒˆã‚’å–å¾—ï¼ˆconstants.pyã‹ã‚‰ç›´æ¥ä½¿ç”¨ï¼‰
            from app.config.constants import PARENT_ACTION_OPTIONS
            selected_parent_action_detail = next(
                (opt for opt in PARENT_ACTION_OPTIONS if opt["text"] == parent_action),
                None
            )

            if selected_parent_action_detail is None:
                logger.error(f"Invalid parent action selected: {parent_action}")
                return None

            # AIäººæ ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‚·ãƒãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚¬ã‚¤ãƒ‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ„ã¿åˆã‚ã›ã‚‹
            persona_prompt = AI_PERSONAS[ai_mode]
            guide_prompt_content = GUIDE_SYSTEM_PROMPT(
                event_name=event_name,
                scene_description=scene_description,
                child_action=child_action,
                parent_action_text=selected_parent_action_detail["text"],
                evaluation=selected_parent_action_detail["evaluation"],
                ai_hint=selected_parent_action_detail["ai_hint"]
            )

            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’èª¿æ•´
            user_message = f"""
ã‚¤ãƒ™ãƒ³ãƒˆ: {event_name}
ã‚·ãƒ¼ãƒ³: {scene_description}
å­ã©ã‚‚ã®è¡Œå‹•: {child_action}
ä¿è­·è€…ã®è¡Œå‹•: {parent_action}
ã“ã®çŠ¶æ³ã§ã®ç§ï¼ˆä¿è­·è€…ï¼‰ã®è¡Œå‹•ã«ã¤ã„ã¦ã€{ai_mode}ã®è¦–ç‚¹ã‹ã‚‰å…·ä½“çš„ãªã‚¬ã‚¤ãƒ‰ã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
            """

            messages = [
                {"role": "system", "content": persona_prompt},
                {"role": "user", "content": guide_prompt_content},
                {"role": "user", "content": user_message}
            ]

            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=Settings.MAX_TOKENS * 3, # é•·ã‚ã®å›ç­”ã‚’æƒ³å®š
                temperature=Settings.TEMPERATURE
            )

            guide_answer = response.choices[0].message.content
            logger.info(f"Generated situation guide for event: {event_name}, child_action: {child_action}")
            return guide_answer

        except Exception as e:
            logger.error(f"Error generating situation guide: {e}")
            ErrorHandler.handle_api_error(e)
            return None

    def get_situation_guide_stream(self, event_name: str, scene_description: str,
                                   child_action: str, parent_action: str, ai_mode: str) -> Generator[str, None, None]:
        """
        ä¿è­·è€…å‘ã‘ã‚·ãƒãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³åˆ¥ã‚¬ã‚¤ãƒ‰ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ
        """
        try:
            if ai_mode not in AI_PERSONAS:
                logger.error(f"Invalid AI mode: {ai_mode}")
                yield ""
                return

            from app.config.constants import PARENT_ACTION_OPTIONS
            selected_parent_action_detail = next(
                (opt for opt in PARENT_ACTION_OPTIONS if opt["text"] == parent_action),
                None
            )

            if selected_parent_action_detail is None:
                logger.error(f"Invalid parent action selected: {parent_action}")
                yield ""
                return

            persona_prompt = AI_PERSONAS[ai_mode]
            guide_prompt_content = GUIDE_SYSTEM_PROMPT(
                event_name=event_name,
                scene_description=scene_description,
                child_action=child_action,
                parent_action_text=selected_parent_action_detail["text"],
                evaluation=selected_parent_action_detail["evaluation"],
                ai_hint=selected_parent_action_detail["ai_hint"]
            )

            user_message = f"""
ã‚¤ãƒ™ãƒ³ãƒˆ: {event_name}
ã‚·ãƒ¼ãƒ³: {scene_description}
å­ã©ã‚‚ã®è¡Œå‹•: {child_action}
ä¿è­·è€…ã®è¡Œå‹•: {parent_action}
ã“ã®çŠ¶æ³ã§ã®ç§ï¼ˆä¿è­·è€…ï¼‰ã®è¡Œå‹•ã«ã¤ã„ã¦ã€{ai_mode}ã®è¦–ç‚¹ã‹ã‚‰å…·ä½“çš„ãªã‚¬ã‚¤ãƒ‰ã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
            """

            messages = [
                {"role": "system", "content": persona_prompt},
                {"role": "user", "content": guide_prompt_content},
                {"role": "user", "content": user_message}
            ]

            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=Settings.MAX_TOKENS * 3,
                temperature=Settings.TEMPERATURE,
                stream=True
            )

            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content

            logger.info(f"Generated streaming situation guide for event: {event_name}, child_action: {child_action}")

        except Exception as e:
            logger.error(f"Error generating streaming situation guide: {e}")
            ErrorHandler.handle_api_error(e)
            yield ""
    
    def get_parent_advice(self, question: str, context: str, ai_mode: str) -> Optional[str]:
        """
        ä¿è­·è€…å‘ã‘ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ç”Ÿæˆï¼ˆã‚·ãƒãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³åˆ¥ã‚¬ã‚¤ãƒ‰ç”¨ï¼‰
        
        Args:
            question: ä¿è­·è€…ã‹ã‚‰ã®è³ªå•
            context: çŠ¶æ³ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            ai_mode: AIäººæ ¼ãƒ¢ãƒ¼ãƒ‰
            
        Returns:
            AIãŒç”Ÿæˆã—ãŸã‚¢ãƒ‰ãƒã‚¤ã‚¹æ–‡å­—åˆ—ã€‚å¤±æ•—æ™‚ã¯None
        """
        try:
            if ai_mode not in AI_PERSONAS:
                logger.error(f"Invalid AI mode: {ai_mode}")
                return None
            
            system_prompt = AI_PERSONAS[ai_mode]
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨è³ªå•ã‚’çµ„ã¿åˆã‚ã›ã‚‹
            full_message = f"{context}\n\nè³ªå•: {question}"
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": full_message}
                ],
                max_tokens=Settings.MAX_TOKENS * 2,
                temperature=Settings.TEMPERATURE
            )
            
            advice = response.choices[0].message.content
            logger.info(f"Generated parent advice in mode: {ai_mode}")
            return advice
            
        except Exception as e:
            logger.error(f"Error generating parent advice: {e}")
            ErrorHandler.handle_api_error(e)
            return None
    
    def generate_parent_action_feedback(
        self, 
        event: str, 
        child_action: str, 
        parent_action: str, 
        evaluation: str,
        ai_mode: str = "ğŸ©º ãƒ­ã‚¸ã‚«ãƒ«ãƒ‰ã‚¯ã‚¿ãƒ¼",
        detail_level: str = "brief"
    ) -> Optional[str]:
        """
        ä¿è­·è€…ã®å¯¾å¿œé¸æŠã«å¯¾ã™ã‚‹AIãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆãƒ»è©³ç´°ç‰ˆï¼‰
        
        Args:
            event: ã‚¤ãƒ™ãƒ³ãƒˆåï¼ˆä¾‹ï¼šã€ŒåºŠå±‹ã€ï¼‰
            child_action: å­ã©ã‚‚ã®è¡Œå‹•ï¼ˆä¾‹ï¼šã€Œãƒãƒªã‚«ãƒ³ã®éŸ³ã‚’èã„ã¦ãƒ‘ãƒ‹ãƒƒã‚¯ã«ãªã‚‹ã€ï¼‰
            parent_action: ä¿è­·è€…ãŒé¸æŠã—ãŸå¯¾å¿œï¼ˆä¾‹ï¼šã€Œäº‹å‰ã«äºˆå‘Šã—ã€ã‚¤ãƒ¤ãƒ¼ãƒãƒ•ã®ä½¿ç”¨ã‚’ææ¡ˆã™ã‚‹ã€ï¼‰
            evaluation: è©•ä¾¡ï¼ˆappropriate/acceptable/inappropriateï¼‰
            ai_mode: AIäººæ ¼ãƒ¢ãƒ¼ãƒ‰
            detail_level: "brief"ï¼ˆç°¡æ˜“ï¼‰ã¾ãŸã¯ "detailed"ï¼ˆè©³ç´°ï¼‰
            
        Returns:
            AIãŒç”Ÿæˆã—ãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ–‡å­—åˆ—ã€‚å¤±æ•—æ™‚ã¯None
        """
        try:
            if ai_mode not in AI_PERSONAS:
                logger.error(f"Invalid AI mode: {ai_mode}")
                return None
            
            from app.config.prompts import get_parent_action_feedback_prompt
            
            system_prompt = get_parent_action_feedback_prompt(
                event=event,
                child_action=child_action,
                parent_action=parent_action,
                evaluation=evaluation,
                ai_mode=ai_mode,
                detail_level=detail_level
            )
            
            user_message = f"""
ã‚¤ãƒ™ãƒ³ãƒˆ: {event}
å­ã©ã‚‚ã®è¡Œå‹•: {child_action}
ä¿è­·è€…ã®å¯¾å¿œ: {parent_action}
è©•ä¾¡: {evaluation}

ã“ã®ä¿è­·è€…ã®å¯¾å¿œã«ã¤ã„ã¦ã€{detail_level}ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
            """
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=Settings.MAX_TOKENS if detail_level == "brief" else Settings.MAX_TOKENS * 3,
                temperature=Settings.TEMPERATURE
            )
            
            feedback = response.choices[0].message.content
            logger.info(f"Generated parent action feedback ({detail_level}) for event: {event}")
            return feedback
            
        except Exception as e:
            logger.error(f"Error generating parent action feedback: {e}")
            ErrorHandler.handle_api_error(e)
            return None
    
    def generate_parent_action_feedback_stream(
        self, 
        event: str, 
        child_action: str, 
        parent_action: str, 
        evaluation: str,
        ai_mode: str = "ğŸ©º ãƒ­ã‚¸ã‚«ãƒ«ãƒ‰ã‚¯ã‚¿ãƒ¼",
        detail_level: str = "brief",
        rag_context: Optional[str] = None
    ) -> Generator[str, None, None]:
        """
        ä¿è­·è€…ã®å¯¾å¿œé¸æŠã«å¯¾ã™ã‚‹AIãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆï¼ˆRAGå¯¾å¿œæº–å‚™æ¸ˆã¿ï¼‰
        
        Args:
            event: ã‚¤ãƒ™ãƒ³ãƒˆå
            child_action: å­ã©ã‚‚ã®è¡Œå‹•
            parent_action: ä¿è­·è€…ãŒé¸æŠã—ãŸå¯¾å¿œ
            evaluation: è©•ä¾¡
            ai_mode: AIäººæ ¼ãƒ¢ãƒ¼ãƒ‰
            detail_level: "brief"ï¼ˆç°¡æ˜“ï¼‰ã¾ãŸã¯ "detailed"ï¼ˆè©³ç´°ï¼‰
            rag_context: RAGã‹ã‚‰å–å¾—ã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå°†æ¥çš„ã«ä½¿ç”¨ï¼‰
            
        Yields:
            AIãŒç”Ÿæˆã—ãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®æ–­ç‰‡
        """
        try:
            if ai_mode not in AI_PERSONAS:
                logger.error(f"Invalid AI mode: {ai_mode}")
                yield ""
                return
            
            from app.config.prompts import get_parent_action_feedback_prompt
            
            system_prompt = get_parent_action_feedback_prompt(
                event=event,
                child_action=child_action,
                parent_action=parent_action,
                evaluation=evaluation,
                ai_mode=ai_mode,
                detail_level=detail_level,
                rag_context=rag_context
            )
            
            user_message = f"""
ã‚¤ãƒ™ãƒ³ãƒˆ: {event}
å­ã©ã‚‚ã®è¡Œå‹•: {child_action}
ä¿è­·è€…ã®å¯¾å¿œ: {parent_action}
è©•ä¾¡: {evaluation}

ã“ã®ä¿è­·è€…ã®å¯¾å¿œã«ã¤ã„ã¦ã€{detail_level}ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
            """
            
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=Settings.MAX_TOKENS if detail_level == "brief" else Settings.MAX_TOKENS * 3,
                temperature=Settings.TEMPERATURE,
                stream=True
            )
            
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
            
            logger.info(f"Generated streaming parent action feedback ({detail_level}) for event: {event}")
            
        except Exception as e:
            logger.error(f"Error generating streaming parent action feedback: {e}")
            ErrorHandler.handle_api_error(e)
            yield ""

